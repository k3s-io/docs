"use strict";(self.webpackChunkk_3_s_docs=self.webpackChunkk_3_s_docs||[]).push([[4706],{7660:(e,s,n)=>{n.r(s),n.d(s,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"known-issues","title":"Known Issues","description":"The Known Issues are updated periodically and designed to inform you about any issues that may not be immediately addressed in the next upcoming release.","source":"@site/docs/known-issues.md","sourceDirName":".","slug":"/known-issues","permalink":"/known-issues","draft":false,"unlisted":false,"editUrl":"https://github.com/k3s-io/docs/edit/main/docs/known-issues.md","tags":[],"version":"current","lastUpdatedAt":1757614189000,"frontMatter":{"title":"Known Issues"},"sidebar":"mySidebar","previous":{"title":"Related Projects","permalink":"/related-projects"},"next":{"title":"FAQ","permalink":"/faq"}}');var i=n(4848),r=n(8453);const o={title:"Known Issues"},a=void 0,l={},d=[{value:"Kine/SQL 2147483647 (MAX INT) Revision Limit",id:"kinesql-2147483647-max-int-revision-limit",level:3},{value:"Docker Snap",id:"docker-snap",level:3},{value:"iptables",id:"iptables",level:3},{value:"Rootless Mode",id:"rootless-mode",level:3},{value:"Upgrading Hardened Clusters from v1.24.x to v1.25.x",id:"hardened-125",level:3}];function c(e){const s={a:"a",admonition:"admonition",code:"code",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(s.p,{children:["The Known Issues are updated periodically and designed to inform you about any issues that may not be immediately addressed in the next upcoming release.\nFor the most up-to-date information, check open and pinned issues on the ",(0,i.jsx)(s.a,{href:"https://github.com/k3s-io/k3s/issues",children:"K3s Project Issue Tracker"}),".\nIf you are not running the most recent release of K3s, make sure to also search closed issues and release notes to ensure that your issue has not already been resolved."]}),"\n",(0,i.jsx)(s.h3,{id:"kinesql-2147483647-max-int-revision-limit",children:"Kine/SQL 2147483647 (MAX INT) Revision Limit"}),"\n",(0,i.jsxs)(s.p,{children:["When using K3s with an ",(0,i.jsx)(s.a,{href:"/datastore/ha",children:"external SQL database"})," that was created on a release of K3s prior to May 2024, you must apply schema migrations to your database to allow for more than 2.1 million revisions to be stored within the database.\nDatabases without updated schema will become read-only when the current datastore revision reaches 2147483647."]}),"\n",(0,i.jsxs)(s.p,{children:["You can check the current revision of your datastore by examining the ",(0,i.jsx)(s.code,{children:"resourceVersion"})," field of the response to a list call made against the Kubernetes API.\nFor example, in the following output, the current revision is 12345:"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-bash",children:'$ kubectl get --raw /api/v1/namespaces?labelSelector=none\n{"kind":"NamespaceList","apiVersion":"v1","metadata":{"resourceVersion":"12345"},"items":[]}\n'})}),"\n",(0,i.jsxs)(s.p,{children:["You can update the datastore schema by setting the ",(0,i.jsx)(s.code,{children:"KINE_SCHEMA_MIGRATION"})," environment variable to 1 or higher in the K3s service's env file, and restarting the service.\nThis change should be made on all servers within the cluster."]}),"\n",(0,i.jsx)(s.h3,{id:"docker-snap",children:"Docker Snap"}),"\n",(0,i.jsx)(s.p,{children:"If you plan to use K3s with the Docker container runtime, the Docker snap package is not recommended as it has been known to cause issues running K3s. Install Docker using the native package management system provided by your operating system."}),"\n",(0,i.jsx)(s.h3,{id:"iptables",children:"iptables"}),"\n",(0,i.jsx)(s.p,{children:"If your node uses iptables v1.6.1 or older in nftables mode you might encounter issues. We recommend utilizing newer iptables (such as 1.6.1+), or running iptables legacy mode, to avoid issues."}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-bash",children:"update-alternatives --set iptables /usr/sbin/iptables-legacy\nupdate-alternatives --set ip6tables /usr/sbin/ip6tables-legacy\n"})}),"\n",(0,i.jsxs)(s.p,{children:["Iptables versions 1.8.0-1.8.4 also have known issues that can cause K3s to fail. Several popular Linux distributions ship with these versions by default. One bug causes the accumulation of duplicate rules, which negatively affects the performance and stability of the node. See ",(0,i.jsx)(s.a,{href:"https://github.com/k3s-io/k3s/issues/3117",children:"Issue #3117"})," for information on how to determine if you are affected by this problem."]}),"\n",(0,i.jsxs)(s.p,{children:["K3s includes a known-good version of iptables (v1.8.8) which has been tested to function properly. You can tell K3s to use its bundled version of iptables by starting K3s with the ",(0,i.jsx)(s.code,{children:"--prefer-bundled-bin"})," option, or by uninstalling the iptables/nftables packages from your operating system."]}),"\n",(0,i.jsx)(s.admonition,{title:"Version Gate",type:"info",children:(0,i.jsxs)(s.p,{children:["The ",(0,i.jsx)(s.code,{children:"--prefer-bundled-bin"})," flag is available starting with the 2022-12 releases (v1.26.0+k3s1, v1.25.5+k3s1, v1.24.9+k3s1, v1.23.15+k3s1)."]})}),"\n",(0,i.jsx)(s.h3,{id:"rootless-mode",children:"Rootless Mode"}),"\n",(0,i.jsxs)(s.p,{children:["Running K3s with Rootless mode is experimental and has several ",(0,i.jsx)(s.a,{href:"/advanced#known-issues-with-rootless-mode",children:"known issues."})]}),"\n",(0,i.jsx)(s.h3,{id:"hardened-125",children:"Upgrading Hardened Clusters from v1.24.x to v1.25.x"}),"\n",(0,i.jsxs)(s.p,{children:["Kubernetes removed PodSecurityPolicy from v1.25 in favor of Pod Security Standards. You can read more about PSS in the ",(0,i.jsx)(s.a,{href:"https://kubernetes.io/docs/concepts/security/pod-security-standards/",children:"upstream documentation"}),". For K3S, there are some manual steps that must be taken if any ",(0,i.jsx)(s.code,{children:"PodSecurityPolicy"})," has been configured on the nodes."]}),"\n",(0,i.jsxs)(s.ol,{children:["\n",(0,i.jsxs)(s.li,{children:["On all nodes, update the ",(0,i.jsx)(s.code,{children:"kube-apiserver-arg"})," value to remove the ",(0,i.jsx)(s.code,{children:"PodSecurityPolicy"})," admission-plugin. Add the following arg value instead: ",(0,i.jsx)(s.code,{children:"'admission-control-config-file=/var/lib/rancher/k3s/server/psa.yaml'"}),", but do NOT restart or upgrade K3S yet. Below is an example of what a configuration file might look like after this update for the node to be hardened:"]}),"\n"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-yaml",children:"protect-kernel-defaults: true\nsecrets-encryption: true\nkube-apiserver-arg:\n  - 'admission-control-config-file=/var/lib/rancher/k3s/server/psa.yaml'\n  - 'audit-log-path=/var/lib/rancher/k3s/server/logs/audit.log'\n  - 'audit-policy-file=/var/lib/rancher/k3s/server/audit.yaml'\n  - 'audit-log-maxage=30'\n  - 'audit-log-maxbackup=10'\n  - 'audit-log-maxsize=100'\nkube-controller-manager-arg:\n  - 'terminated-pod-gc-threshold=10'\n  - 'use-service-account-credentials=true'\nkubelet-arg:\n  - 'streaming-connection-idle-timeout=5m'\n"})}),"\n",(0,i.jsxs)(s.ol,{start:"2",children:["\n",(0,i.jsxs)(s.li,{children:["Create the ",(0,i.jsx)(s.code,{children:"/var/lib/rancher/k3s/server/psa.yaml"})," file with the following contents. You may want to exempt more namespaces as well. The below example exempts ",(0,i.jsx)(s.code,{children:"kube-system"})," (required), ",(0,i.jsx)(s.code,{children:"cis-operator-system"})," (optional, but useful for when running security scans through Rancher), and ",(0,i.jsx)(s.code,{children:"system-upgrade"})," (required if doing ",(0,i.jsx)(s.a,{href:"/upgrades/automated",children:"Automated Upgrades"}),")."]}),"\n"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-yaml",children:'apiVersion: apiserver.config.k8s.io/v1\nkind: AdmissionConfiguration\nplugins:\n- name: PodSecurity\n  configuration:\n    apiVersion: pod-security.admission.config.k8s.io/v1beta1\n    kind: PodSecurityConfiguration\n    defaults:\n      enforce: "restricted"\n      enforce-version: "latest"\n      audit: "restricted"\n      audit-version: "latest"\n      warn: "restricted"\n      warn-version: "latest"\n    exemptions:\n      usernames: []\n      runtimeClasses: []\n      namespaces: [kube-system, cis-operator-system, system-upgrade]\n'})}),"\n",(0,i.jsxs)(s.ol,{start:"3",children:["\n",(0,i.jsxs)(s.li,{children:["Perform the upgrade as normal. If doing ",(0,i.jsx)(s.a,{href:"/upgrades/automated",children:"Automated Upgrades"}),", ensure that the namespace where the ",(0,i.jsx)(s.code,{children:"system-upgrade-controller"})," pod is running in is setup to be privileged in accordance with the ",(0,i.jsx)(s.a,{href:"https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-levels",children:"Pod Security levels"}),":"]}),"\n"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-yaml",children:"apiVersion: v1\nkind: Namespace\nmetadata:\n  name: system-upgrade\n  labels:\n    # This value must be privileged for the controller to run successfully.\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/enforce-version: v1.25\n    # We are setting these to our _desired_ `enforce` level, but note that these below values can be any of the available options.\n    pod-security.kubernetes.io/audit: privileged\n    pod-security.kubernetes.io/audit-version: v1.25\n    pod-security.kubernetes.io/warn: privileged\n    pod-security.kubernetes.io/warn-version: v1.25\n"})}),"\n",(0,i.jsxs)(s.ol,{start:"4",children:["\n",(0,i.jsxs)(s.li,{children:["After the upgrade is complete, remove any remaining PSP resources from the cluster. In many cases, there may be PodSecurityPolicies and associated RBAC resources in custom files used for hardening within ",(0,i.jsx)(s.code,{children:"/var/lib/rancher/k3s/server/manifests/"}),". Remove those resources and k3s will update automatically. Sometimes, due to timing, some of these may be left in the cluster, in which case you will need to delete them manually. If the ",(0,i.jsx)(s.a,{href:"/security/hardening-guide",children:"Hardening Guide"})," was previously followed, you should be able to delete them via the following:"]}),"\n"]}),"\n",(0,i.jsx)(s.pre,{children:(0,i.jsx)(s.code,{className:"language-sh",children:"# Get the resources associated with PSPs\n$ kubectl get roles,clusterroles,rolebindings,clusterrolebindings -A | grep -i psp\n\n# Delete those resources:\n$ kubectl delete clusterrole.rbac.authorization.k8s.io/psp:restricted-psp clusterrole.rbac.authorization.k8s.io/psp:svclb-psp clusterrole.rbac.authorization.k8s.io/psp:system-unrestricted-psp clusterrolebinding.rbac.authorization.k8s.io/default:restricted-psp clusterrolebinding.rbac.authorization.k8s.io/system-unrestricted-node-psp-rolebinding && kubectl delete -n kube-system rolebinding.rbac.authorization.k8s.io/svclb-psp-rolebinding rolebinding.rbac.authorization.k8s.io/system-unrestricted-svc-acct-psp-rolebinding\n"})})]})}function u(e={}){const{wrapper:s}={...(0,r.R)(),...e.components};return s?(0,i.jsx)(s,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,s,n)=>{n.d(s,{R:()=>o,x:()=>a});var t=n(6540);const i={},r=t.createContext(i);function o(e){const s=t.useContext(r);return t.useMemo((function(){return"function"==typeof e?e(s):{...s,...e}}),[s,e])}function a(e){let s;return s=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(r.Provider,{value:s},e.children)}}}]);