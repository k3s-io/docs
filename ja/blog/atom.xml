<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://docs.k3s.io/ja/blog</id>
    <title>K3s Blog</title>
    <updated>2025-09-27T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://docs.k3s.io/ja/blog"/>
    <subtitle>K3s Blog</subtitle>
    <icon>https://docs.k3s.io/ja/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Sysbox Runtime With K3s]]></title>
        <id>https://docs.k3s.io/ja/blog/2025/09/27/k3s-sysbox</id>
        <link href="https://docs.k3s.io/ja/blog/2025/09/27/k3s-sysbox"/>
        <updated>2025-09-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Integrating sysbox runtime with k3s' containerd]]></summary>
        <content type="html"><![CDATA[<p>The K3s binary bundles all the components needed to run a production-ready, CNCF-conformant Kubernetes cluster including containerd, runc, kubelet, and more. In this post we will discuss how containerd communicates with OCI runtimes and will discuss adding another container runtime (Sysbox) to K3s and how it can be used to run system pods in your environment.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="containerd-and-runc">Containerd and Runc<a href="https://docs.k3s.io/ja/blog/2025/09/27/k3s-sysbox#containerd-and-runc" class="hash-link" aria-label="Containerd and Runc への直接リンク" title="Containerd and Runc への直接リンク" translate="no">​</a></h2>
<p>First we need to talk briefly about how containerd works with runc. Containerd is a long running daemon that is responsible for:</p>
<ul>
<li class=""><strong>Image Management</strong>: Pulls and stores images from registries.</li>
<li class=""><strong>Container Management</strong>: Manages the lifecycle of containers (create, start, stop, delete).</li>
<li class=""><strong>Snapshot Management</strong>: Uses snapshotters to manage the filesystem layers for containers.</li>
<li class=""><strong>Runtime Management</strong>: Delegates the creation of containers to OCI-compatible runtimes like runc.</li>
</ul>
<p>When you create a pod in Kubernetes, kubelet uses the CRI plugin implemented in containerd to request a <strong>pod sandbox</strong> (<code>RunPodSandbox</code>) and then container creation (<code>CreateContainer</code>). Containerd then calls a <code>shim</code> process that acts as a middleman between <code>containerd</code> and the OCI runtime (for example <code>runc</code>). This shim process allows the container to keep running even if the containerd daemon crashes or restarts.</p>
<p>The shim generates the OCI runtime bundle (<code>config.json</code> and <code>rootfs</code> path) and then executes the runc binary. Runc reads the <code>config.json</code>, sets up the container’s namespaces and cgroups, and then launches the container process.</p>
<p>Runc is the component that directly interfaces with the Linux kernel — configuring cgroups, namespaces, seccomp, capabilities, and mounts. After runc finishes creating the container, it <strong>exits</strong>, leaving the shim to manage the lifecycle and I/O of the container.</p>
<!-- -->
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sysbox-runtime">Sysbox Runtime<a href="https://docs.k3s.io/ja/blog/2025/09/27/k3s-sysbox#sysbox-runtime" class="hash-link" aria-label="Sysbox Runtime への直接リンク" title="Sysbox Runtime への直接リンク" translate="no">​</a></h2>
<p><a href="https://github.com/nestybox/sysbox" target="_blank" rel="noopener noreferrer" class="">Sysbox</a> is an open-source, next-generation container runtime created by Nestybox. Unlike traditional runtimes (such as runc), Sysbox is designed to let you run "system containers". It primarily leverages <strong>Linux user namespaces</strong> and other features to provide containers that behave more like lightweight virtual machines.</p>
<p>This means you can run workloads like Docker, Systemd, containerd, or even K3s inside your pods — all without requiring privileged mode.</p>
<p>In short, Sysbox bridges the gap between application containers and virtual machines, enabling use cases like running Kubernetes-in-Kubernetes (K8s-in-K8s), CI/CD pipelines that need full OS-like environments, or development sandboxes with VM-level isolation but container speed.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>備考</div><div class="admonitionContent_BuS1"><p>Currently, Sysbox officially supports <strong>CRI-O</strong> only. CRI-O has native support for Linux user namespaces, which Sysbox relies on. While containerd added user namespace support starting in version v2.0, there was a <a href="https://github.com/nestybox/sysbox/issues/958" target="_blank" rel="noopener noreferrer" class="">bug</a> in sysbox-runc that prevented it from working properly with Sysbox.</p></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="sysbox-runc-containerd-integration">Sysbox-runc Containerd Integration<a href="https://docs.k3s.io/ja/blog/2025/09/27/k3s-sysbox#sysbox-runc-containerd-integration" class="hash-link" aria-label="Sysbox-runc Containerd Integration への直接リンク" title="Sysbox-runc Containerd Integration への直接リンク" translate="no">​</a></h2>
<p>After investigating this issue, I was able to locate the root cause, as explained in this <a href="https://github.com/nestybox/sysbox-runc/pull/106" target="_blank" rel="noopener noreferrer" class="">PR</a>. Containerd was failing to run a specific subcommand for sysbox-runc called <code>features</code>, which led to the following error:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">level=debug msg="failed to introspect features of runtime \"sysbox-runc\"" error="failed to unmarshal Features (*anypb.Any): type with url : not found"</span><br></span></code></pre></div></div>
<p>Because of this, containerd instructed sysbox-runc to run containers <strong>without user namespaces</strong>, causing container creation to fail. The fix for this bug was recently merged in the <code>sysbox-runc</code> repo, enabling containerd to work with sysbox-runc.</p>
<h1>Running Sysbox-runc With K3S</h1>
<p>In order to run <code>sysbox-runc</code> with K3s you need to have a running K3s cluster, and then you can proceed to install the latest version of sysbox, However, since the fix for containerd support hasn't yet been integrated to sysbox main repo only in <code>sysbox-runc</code>, we need to build the binaries from source to get the latest updates.</p>
<ol>
<li class="">
<p>Install docker in your system.</p>
</li>
<li class="">
<p>Clone the repo and prepare the code</p>
</li>
</ol>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">git clone --recursive https://github.com/nestybox/sysbox.git</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">cd sysbox/sysbox-runc</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">git pull origin main</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">cd ..</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">make IMAGE_BASE_DISTRO=ubuntu IMAGE_BASE_RELEASE=jammy sysbox-static</span><br></span></code></pre></div></div>
<p>You can then copy the binaries built to <code>/usr/bin</code> or if you are building on the same machine that you will run containerd you can just run:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">make install</span><br></span></code></pre></div></div>
<ol start="3">
<li class="">Run sysbox binary</li>
</ol>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">sysbox</span><br></span></code></pre></div></div>
<ol start="4">
<li class="">Create sysbox runc runtime class</li>
</ol>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">apiVersion: node.k8s.io/v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">handler: sysbox-runc</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">kind: RuntimeClass</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">metadata:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  name: sysbox-runc</span><br></span></code></pre></div></div>
<ol start="5">
<li class="">Add sysbox runc to containerd configuration, you can do that by creating <code>/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl</code>:</li>
</ol>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.sysbox-runc]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  runtime_type = "io.containerd.runc.v2"</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">[plugins.'io.containerd.cri.v1.runtime'.containerd.runtimes.sysbox-runc.options]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  SystemdCgroup = false</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  BinaryName="/usr/bin/sysbox-runc"</span><br></span></code></pre></div></div>
<ol start="6">
<li class="">Finally you can create pod running with the runtime class for sysbox-runc and <code>hostUsers: false</code>:</li>
</ol>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">apiVersion: v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">kind: Pod</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">metadata:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  name: ubuntu</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">spec:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  runtimeClassName: sysbox-runc</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  hostUsers: false</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  containers:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  - name: ubuntu2204</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    image: ubuntu:22.04</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    command: ["sleep", "40000000000"]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  restartPolicy: Never</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion">Conclusion<a href="https://docs.k3s.io/ja/blog/2025/09/27/k3s-sysbox#conclusion" class="hash-link" aria-label="Conclusion への直接リンク" title="Conclusion への直接リンク" translate="no">​</a></h2>
<p>Sysbox brings a powerful capability to Kubernetes: the ability to run system-level workloads inside containers with strong isolation, without requiring privileged mode. When combined with K3s, this opens the door to new use cases such as:</p>
<ul>
<li class="">Running Kubernetes-in-Kubernetes clusters for virtual clusters (<a href="https://github.com/rancher/k3k" target="_blank" rel="noopener noreferrer" class="">k3k</a>).</li>
<li class="">Creating secure developer sandboxes that behave like lightweight VMs.</li>
<li class="">Running system daemons or nested container engines inside pods.</li>
</ul>
<p>While Sysbox is officially supported with CRI-O today, the recent fixes in <code>sysbox-runc</code> allow it to run on containerd as well — making it possible to integrate with K3s. The integration is still evolving, but it shows how the container ecosystem is moving beyond traditional app containers toward more flexible "system containers."</p>
<p>If you’re experimenting with K3s and want to explore system workloads inside pods, Sysbox provides a compelling way to do so while maintaining Kubernetes-native workflows</p>]]></content>
        <author>
            <name>Hussein Galal</name>
            <uri>https://github.com/galal-hussein</uri>
        </author>
        <category label="runc" term="runc"/>
        <category label="k3s" term="k3s"/>
        <category label="sysbox" term="sysbox"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kubernetes v1.34 is out!]]></title>
        <id>https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release</id>
        <link href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release"/>
        <updated>2025-08-30T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Step back and look at K3s recent work and achievements]]></summary>
        <content type="html"><![CDATA[<p>Kubernetes 1.34 is finally here! A look back at K3s recent work and achievements 🥳</p>
<p>It's been a busy and productive few months, and we're excited to share some of the amazing progress we've made. From new features that make your lives easier to important bug fixes and foundational improvements, our team and community have been hard at work.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="key-features-and-improvements-">Key Features and Improvements ✨<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#key-features-and-improvements-" class="hash-link" aria-label="Key Features and Improvements ✨ への直接リンク" title="Key Features and Improvements ✨ への直接リンク" translate="no">​</a></h2>
<p>Here’s a look at some of the most impactful features and updates we've rolled out:</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="automatic-certificate-renewal-window-increase-">Automatic Certificate Renewal Window Increase 📅<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#automatic-certificate-renewal-window-increase-" class="hash-link" aria-label="Automatic Certificate Renewal Window Increase 📅 への直接リンク" title="Automatic Certificate Renewal Window Increase 📅 への直接リンク" translate="no">​</a></h4>
<p>We've made certificate management more user-friendly by increasing the automatic certificate renewal window from 90 to 120 days. This means if you perform quarterly upgrades, your certificates will be renewed more frequently, preventing them from expiring and making your clusters unusable. For more information check our <a href="https://docs.k3s.io/cli/certificate#client-and-server-certificates" target="_blank" rel="noopener noreferrer" class="">docs</a></p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="optional-airgap-image-tarball-imports-">Optional Airgap Image Tarball Imports 💨<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#optional-airgap-image-tarball-imports-" class="hash-link" aria-label="Optional Airgap Image Tarball Imports 💨 への直接リンク" title="Optional Airgap Image Tarball Imports 💨 への直接リンク" translate="no">​</a></h4>
<p>For those who use K3s in an airgap environment, you now have the option to skip importing all image tarballs. You can select to import only images that have changed since they were last imported, even across restarts. This can significantly speed up the startup process when deploying a large number of images, as the kubelet can start sooner. Check our <a href="https://docs.k3s.io/installation/airgap?airgap-load-images=Manually+Deploy+Images#enable-conditional-image-imports" target="_blank" rel="noopener noreferrer" class="">docs</a> for more information</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="enhanced-certificate-check-output-">Enhanced Certificate Check Output ✅<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#enhanced-certificate-check-output-" class="hash-link" aria-label="Enhanced Certificate Check Output ✅ への直接リンク" title="Enhanced Certificate Check Output ✅ への直接リンク" translate="no">​</a></h4>
<p>We've improved the output of our certificate checks to provide clearer information about certificate usage and expiration dates. The output can also now be viewed in different formats. Our <a href="https://docs.k3s.io/cli/certificate#checking-expiration-dates" target="_blank" rel="noopener noreferrer" class="">docs</a> show an example of the new output format.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="kube-scheduler-and-kube-controller-manager-certificate-management-">Kube-scheduler and Kube-controller-manager Certificate Management 🔐<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#kube-scheduler-and-kube-controller-manager-certificate-management-" class="hash-link" aria-label="Kube-scheduler and Kube-controller-manager Certificate Management 🔐 への直接リンク" title="Kube-scheduler and Kube-controller-manager Certificate Management 🔐 への直接リンク" translate="no">​</a></h4>
<p>We are now generating and managing certificates for kube-scheduler and kube-controller-manager, and they can be rotated using our existing certificate rotation <a href="https://docs.k3s.io/cli/certificate" target="_blank" rel="noopener noreferrer" class="">tool</a>.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="retention-flag-for-s3-stored-snapshots-">Retention Flag for S3 Stored Snapshots 💾<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#retention-flag-for-s3-stored-snapshots-" class="hash-link" aria-label="Retention Flag for S3 Stored Snapshots 💾 への直接リンク" title="Retention Flag for S3 Stored Snapshots 💾 への直接リンク" translate="no">​</a></h4>
<p>We've added a new retention flag for snapshots stored in an S3 bucket. This allows you to keep snapshots in S3 for longer periods while maintaining a smaller number of local snapshots. This is a great feature for balancing long-term disaster recovery with local storage efficiency. Check out our <a href="https://docs.k3s.io/cli/etcd-snapshot#s3-retention" target="_blank" rel="noopener noreferrer" class="">docs</a> for more information about how to use it.</p>
<h4 class="anchor anchorTargetStickyNavbar_Vzrq" id="official-governance-model-">Official Governance Model 🤝<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#official-governance-model-" class="hash-link" aria-label="Official Governance Model 🤝 への直接リンク" title="Official Governance Model 🤝 への直接リンク" translate="no">​</a></h4>
<p>We're proud to announce that we now have an official governance model in place! This will help bring more clarity to our project and hopefully encourage more developers to join our fantastic community. You can read it <a href="https://github.com/k3s-io/k3s/blob/master/GOVERNANCE.md" target="_blank" rel="noopener noreferrer" class="">here</a>.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="bug-fixes-and-other-notable-changes-️">Bug Fixes and Other Notable Changes 🛠️<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#bug-fixes-and-other-notable-changes-%EF%B8%8F" class="hash-link" aria-label="Bug Fixes and Other Notable Changes 🛠️ への直接リンク" title="Bug Fixes and Other Notable Changes 🛠️ への直接リンク" translate="no">​</a></h2>
<p>We've also been busy tackling a number of bugs, tech debt items and making other important improvements under the hood:</p>
<p>Numerous bug fixes, including those for secrets encryption timeouts and race conditions, DNS fallbacks, various authorization and authentication handling issues and replacing go-bindata with the go native embed package.</p>
<p>Improvements to our test and build infrastructure, e.g. migrating of K3s release artifacts to GitHub Actions (GHA).</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="version-bumps-for-key-components-">Version bumps for key components 🚀<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#version-bumps-for-key-components-" class="hash-link" aria-label="Version bumps for key components 🚀 への直接リンク" title="Version bumps for key components 🚀 への直接リンク" translate="no">​</a></h2>
<p>Apart from Kubernetes v1.34.x, we bumped versions for several key components. Here is the list with the latest versions:</p>
<table><thead><tr><th style="text-align:left">Component</th><th style="text-align:left">New Version</th></tr></thead><tbody><tr><td style="text-align:left">Kine</td><td style="text-align:left">v0.14.0</td></tr><tr><td style="text-align:left">SQLite</td><td style="text-align:left">v3.50.4</td></tr><tr><td style="text-align:left">Etcd</td><td style="text-align:left">v3.6.4</td></tr><tr><td style="text-align:left">Containerd</td><td style="text-align:left">v2.1.4</td></tr><tr><td style="text-align:left">Runc</td><td style="text-align:left">v1.3.1</td></tr><tr><td style="text-align:left">Flannel</td><td style="text-align:left">v0.27.0</td></tr><tr><td style="text-align:left">Metrics-server</td><td style="text-align:left">v0.8.0</td></tr><tr><td style="text-align:left">Traefik</td><td style="text-align:left">v3.3.6</td></tr><tr><td style="text-align:left">Coredns</td><td style="text-align:left">v1.12.3</td></tr><tr><td style="text-align:left">Helm-controller</td><td style="text-align:left">v0.16.13</td></tr><tr><td style="text-align:left">Local-path-provisioner</td><td style="text-align:left">v0.0.32</td></tr></tbody></table>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="special-thanks-to-our-contributors-">Special Thanks to Our Contributors 🙏<a href="https://docs.k3s.io/ja/blog/2025/08/30/K3s-1.34-release#special-thanks-to-our-contributors-" class="hash-link" aria-label="Special Thanks to Our Contributors 🙏 への直接リンク" title="Special Thanks to Our Contributors 🙏 への直接リンク" translate="no">​</a></h2>
<p>We want to give a special shout-out to the incredible contributors who are not part of our core maintainers list. Your work is invaluable to the project's success. Thank you to: @ErikJiang, @eggplants, @muicoder, @eugercek, @yulken, @l2dy, @OrlinVasilev</p>
<p>We look forward to an even more productive future with all of you!</p>]]></content>
        <author>
            <name>Manuel Buil</name>
            <uri>https://github.com/manuelbuil</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[K3s initialization deep dive]]></title>
        <id>https://docs.k3s.io/ja/blog/2025/03/25/K3s-initialization</id>
        <link href="https://docs.k3s.io/ja/blog/2025/03/25/K3s-initialization"/>
        <updated>2025-03-25T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Explain k3s initialization steps]]></summary>
        <content type="html"><![CDATA[<p>K3s is a lightweight Kubernetes distribution which excels in its deployment speed and minimal resource footprint. In fact, a lot of our users love K3s because it offers an unparalleled initialization speed.</p>
<p>This blog post delves into the heart of K3s's efficiency: its initialization process. We'll embark on a journey through the steps that enable K3s to materialize a fully functional Kubernetes cluster so quickly. By examining K3s’s own logs, we'll unravel the meaning behind each step, providing you with a practical understanding of how K3s achieves its remarkable speed. This exploration not only illuminates the inner workings of K3s but also equips you with the knowledge to troubleshoot your deployments.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-embedded-powerhouse-️">The Embedded Powerhouse ⚙️⚡<a href="https://docs.k3s.io/ja/blog/2025/03/25/K3s-initialization#the-embedded-powerhouse-%EF%B8%8F" class="hash-link" aria-label="The Embedded Powerhouse ⚙️⚡ への直接リンク" title="The Embedded Powerhouse ⚙️⚡ への直接リンク" translate="no">​</a></h2>
<p>K3s leverages <code>go-bindata</code> to embed essential Linux userspace binaries and manifests directly into its executable. This eliminates external dependencies and streamlines the deployment process. Within the K3s binary, you'll find core components like <code>runc</code> and <code>containerd</code>, along with the k3s-root tarball (e.g. k3s-root-amd64.tar). This tarball contains all the userspace binaries necessary for K3s to function, reducing the reliance on the host OS. If you would like all the K3s embedded binaries to take preference over the host OS binaries, you should use the <code>--prefer-bundled-bin</code> flag.</p>
<p>The embedded binaries are always deployed in the same directory: <code>/var/lib/rancher/k3s/data</code>. If you inspect this folder, you will notice it contains at least three subdirectories: <code>cni</code>, <code>current</code> and a long string of characters (or SHA). That long string of characters is generated when building K3s and it is the result of a <code>sha256sum</code> operation made on the tarball with the embedded binaries. As these change in each release, you will see a different string of characters for each release. In fact, after an upgrade, there will be two directories with a long string of characters as their name.</p>
<p><code>current</code> is just a symlink to the SHA directory and <code>cni</code> includes different cni plugins that are also symlinks to the cni binary in the SHA directory. This is because we are building all cni plugins in just one binary using multi-exec tooling. This is again a way to be more efficient and less resource consuming. If your current K3s deployment underwent an upgrade process, you will see one extra directory called <code>previous</code>, which is another symlink to the previous SHA directory. For clarification this example:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">$&gt; ls -ahltr /var/lib/rancher/k3s/data/</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">total 24K</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">-rw------- 1 root root    0 Mar 19 06:22 .lock</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">drwxr-xr-x 4 root root 4.0K Mar 19 06:22 ..</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">drwxr-xr-x 4 root root 4.0K Mar 19 06:28 82142f5157c67effc219aeefe0bc03e0460fc62b9fbae9e901270c86b5635d53</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">lrwxrwxrwx 1 root root   90 Mar 19 06:28 previous -&gt; /var/lib/rancher/k3s/data/82142f5157c67effc219aeefe0bc03e0460fc62b9fbae9e901270c86b5635d53</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">drwxr-xr-x 4 root root 4.0K Mar 19 06:30 b13851fe661ab93938fc9a881cdce529da8c6b9b310b2440ef01a860f8b9c3a9</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">lrwxrwxrwx 1 root root   90 Mar 19 06:30 current -&gt; /var/lib/rancher/k3s/data/b13851fe661ab93938fc9a881cdce529da8c6b9b310b2440ef01a860f8b9c3a9</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">drwxr-xr-x 2 root root 4.0K Mar 19 06:30 cni</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">drwxr-xr-x 4 root root 4.0K Mar 19 06:40 .</span><br></span></code></pre></div></div>
<p>Additionally, K3s includes embedded Helm charts and manifests for deploying critical services such as CoreDNS, Traefik, and local storage. These embedded charts, formatted as yaml files, can be found in the control plane nodes in the directory: <code>/var/lib/rancher/k3s/server/manifests</code>.</p>
<p>For 67MB our K3s binary includes a lot of stuff!</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="the-boot-sequence-step-by-step-">The Boot Sequence, Step-by-Step 👣<a href="https://docs.k3s.io/ja/blog/2025/03/25/K3s-initialization#the-boot-sequence-step-by-step-" class="hash-link" aria-label="The Boot Sequence, Step-by-Step 👣 への直接リンク" title="The Boot Sequence, Step-by-Step 👣 への直接リンク" translate="no">​</a></h2>
<p>Now that we established how K3s is carrying its embedded tools, we can explore the boot up sequence. Let us look at the typical logs you can find in <code>journalctl</code> when deploying a control-plane or K3s server instance. It all starts with:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Starting Lightweight Kubernetes...</span><br></span></code></pre></div></div>
<p>And then:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">/usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service</span><br></span></code></pre></div></div>
<p>This part is checking for a network manager utility which must be disabled as described in the <a href="https://docs.k3s.io/installation/requirements?_highlight=nm&amp;_highlight=cloud&amp;_highlight=setup.service&amp;os=rhel#operating-systems" target="_blank" rel="noopener noreferrer" class="">docs</a>. It configures some parts of the network stack, specifically the routing tables, which conflict with Kubernetes networking, and that is why we verify if it was correctly disabled.</p>
<p>The next message should look familiar</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Acquiring lock file /var/lib/rancher/k3s/data/.lock</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Preparing data dir /var/lib/rancher/k3s/data/f8e9b5e7d85085972f4a9ddfd539d4dcf887be2e380a55f415c93cac5516dad5</span><br></span></code></pre></div></div>
<p>When this message is shown, the directory where K3s deploys the embedded binaries has already been created. At this point, K3s will extract the binaries. We use the lock to avoid concurrent modifications, preventing K3s embedded commands like <code>kubectl</code> or <code>ctr</code> from executing and disturbing the K3s initialization.</p>
<p>The next block of logs point at the K3s version and the datastore. In this case, I am using the default datastore which means kine with sqlite. For more information on the different datastores available check this <a href="https://docs.k3s.io/datastore" target="_blank" rel="noopener noreferrer" class="">link</a></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Starting k3s v1.32.2+k3s1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Configuring database table schema and indexes, this may take a moment...</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Database tables and indexes are up to date</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Kine available at unix://kine.sock</span><br></span></code></pre></div></div>
<p>Once the datastore is available k3s locks the bootstrap key. This step is useful for HA mode and this this key is just a placeholder so that other control-plane nodes do not start generating new CA certs. As we are not using HA mode in this example, this is not relevant.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Bootstrap key locked for initial create</span><br></span></code></pre></div></div>
<p>K3s then generates all the TLS certificates required for the internal communications:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">generated self-signed CA certificate</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generated self-signed CA certificate CN=k3s-server-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=kube-apiserver signed by CN=k3s-server-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generated self-signed CA certificate CN=k3s-request-header-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generated self-signed CA certificate CN=etcd-server-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=etcd-client signed by CN=etcd-server-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generated self-signed CA certificate CN=etcd-peer-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=etcd-peer signed by CN=etcd-peer-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=etcd-server signed by CN=etcd-server-ca@1742309831</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1742309831</span><br></span></code></pre></div></div>
<p>And then saves the bootstrap data in the bootstrap key. Again, this is not relevant for this example:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Saving cluster bootstrap data to datastore</span><br></span></code></pre></div></div>
<p>After that, K3s starts the different Kubernetes components. These components are all run within the k3s process as goroutines, which are lightweight, concurrent functions in Go, allowing for efficient resource usage. This is another design decision taken to reduce boot time and reduce resource consumption.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">Running kube-apiserver</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Running kube-scheduler</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Running kube-controller-manager</span><br></span></code></pre></div></div>
<p>Manifests for packaged components are extracted to <code>/var/lib/rancher/k3s/server/manifests/</code>. When all the different Kubernetes components are running and K3s initialization is ready, the deploy controller begins watching this directory and applies all the manifests. This is how components like CoreDNS or Traefik eventually get installed.</p>
<p>And that’s it, in a short period of time, you end up with a fully deployed and running Kubernetes distribution. 🎉</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion-">Conclusion 🏁<a href="https://docs.k3s.io/ja/blog/2025/03/25/K3s-initialization#conclusion-" class="hash-link" aria-label="Conclusion 🏁 への直接リンク" title="Conclusion 🏁 への直接リンク" translate="no">​</a></h2>
<p>This exploration has hopefully demystified some of the initial steps that enable K3s to materialize a fully functional Kubernetes cluster. By examining the logs, we've shed some light on the meaning behind each step, providing you with a deeper understanding of how K3s deploys in such a fast manner. We hope you find this knowledge useful to troubleshoot or at least to understand a bit deeper how K3s works.</p>]]></content>
        <author>
            <name>Manuel Buil</name>
            <uri>https://github.com/manuelbuil</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Basic HA Cluster]]></title>
        <id>https://docs.k3s.io/ja/blog/2025/03/10/simple-ha</id>
        <link href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha"/>
        <updated>2025-03-10T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Creating the simplest High Availability cluster with LB and upgrading]]></summary>
        <content type="html"><![CDATA[<p>While we have more <a class="" href="https://docs.k3s.io/ja/datastore/ha-embedded">detailed docs</a> on setting up a High Availability (HA) cluster, this post will cover the simplest HA cluster you can create.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="baseline-ha-cluster-️">Baseline HA Cluster 💻🖥️💻<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#baseline-ha-cluster-%EF%B8%8F" class="hash-link" aria-label="Baseline HA Cluster 💻🖥️💻 への直接リンク" title="Baseline HA Cluster 💻🖥️💻 への直接リンク" translate="no">​</a></h2>
<p>Whenever we get a question around HA, this is the cluster configuration I start with. It provides a solid foundation when deploying beyond a single server.</p>
<p>Our cluster will have:</p>
<ul>
<li class="">4 nodes or VMs:<!-- -->
<ul>
<li class="">1 load balancer</li>
<li class="">3 servers</li>
</ul>
</li>
<li class="">A k3s-upgrade plan that will automatically update the cluster to the latest patch version of a given minor.</li>
</ul>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="cluster-setup-">Cluster Setup 🌐🔧<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#cluster-setup-" class="hash-link" aria-label="Cluster Setup 🌐🔧 への直接リンク" title="Cluster Setup 🌐🔧 への直接リンク" translate="no">​</a></h2>
<p>I'm using <code>vagrant</code> to provision 4 Ubuntu 24.04 VMs for this setup, all on a flat network. Setup of nodes is left as an exercise for the reader 😅.</p>
<p>My nodes are configured with the following names and IPs:</p>
<table><thead><tr><th>Name</th><th>IP</th></tr></thead><tbody><tr><td>lb-0</td><td>10.10.10.100</td></tr><tr><td>server-0</td><td>10.10.10.50</td></tr><tr><td>server-1</td><td>10.10.10.51</td></tr><tr><td>server-2</td><td>10.10.10.52</td></tr></tbody></table>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="load-balancer">Load Balancer<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#load-balancer" class="hash-link" aria-label="Load Balancer への直接リンク" title="Load Balancer への直接リンク" translate="no">​</a></h3>
<p>I'm using <a href="https://www.haproxy.org/" target="_blank" rel="noopener noreferrer" class="">haproxy</a> as it supports later expansion to multiple LB nodes (via keepalived).</p>
<p>SSH into the load balancer and install haproxy:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)">sudo</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">apt</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">install</span><span class="token plain"> haproxy</span><br></span></code></pre></div></div>
<p>The haproxy config is simple, just forward traffic to the servers:</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">#/etc/haproxy/haproxy.cfg</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">frontend k3s</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    bind *:6443</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    mode tcp</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    default_backend k3s</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">backend k3s</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    mode tcp</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    option tcp-check</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    balance roundrobin</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    server server-0 10.10.10.50:6443 check</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    server server-1 10.10.10.51:6443 check</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    server server-2 10.10.10.52:6443 check</span><br></span></code></pre></div></div>
<p>Restart haproxy to apply the config:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">systemctl restart haproxy</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="install-k3s-on-first-server">Install K3s on first server<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#install-k3s-on-first-server" class="hash-link" aria-label="Install K3s on first server への直接リンク" title="Install K3s on first server への直接リンク" translate="no">​</a></h3>
<p>On the first server, install K3s with embedded etcd and a known token:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-sfL</span><span class="token plain"> https://get.k3s.io </span><span class="token operator" style="color:rgb(137, 221, 255)">|</span><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">INSTALL_K3S_CHANNEL</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">v1.31 </span><span class="token function" style="color:rgb(130, 170, 255)">sh</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-s</span><span class="token plain"> - </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">--cluster-init </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">--token</span><span class="token plain"> k3sblog --tls-san </span><span class="token number" style="color:rgb(247, 140, 108)">10.10</span><span class="token plain">.10.100</span><br></span></code></pre></div></div>
<p>We pass the <code>--tls-san</code> flag adds the load balancer IP as a Subject Alternative Name (SAN) for the certificate.</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="join-the-other-servers">Join the other servers<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#join-the-other-servers" class="hash-link" aria-label="Join the other servers への直接リンク" title="Join the other servers への直接リンク" translate="no">​</a></h3>
<p>On the other servers, join the cluster via the load balancer:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)">curl</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-sfL</span><span class="token plain"> https://get.k3s.io </span><span class="token operator" style="color:rgb(137, 221, 255)">|</span><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">INSTALL_K3S_CHANNEL</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">v1.31 </span><span class="token function" style="color:rgb(130, 170, 255)">sh</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-s</span><span class="token plain"> - </span><span class="token punctuation" style="color:rgb(199, 146, 234)">\</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token parameter variable" style="color:rgb(191, 199, 213)">--server</span><span class="token plain"> https://10.10.10.100:6443 </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">--token</span><span class="token plain"> k3sblog</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="grab-the-kubeconfig">Grab the kubeconfig<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#grab-the-kubeconfig" class="hash-link" aria-label="Grab the kubeconfig への直接リンク" title="Grab the kubeconfig への直接リンク" translate="no">​</a></h3>
<p>Now that the cluster is up, we can grab the kubeconfig from the first server:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)">scp</span><span class="token plain"> server-0:/etc/rancher/k3s/k3s.yaml k3s.yaml</span><br></span></code></pre></div></div>
<p>Modify it to access the cluster via the load balancer:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)">sed</span><span class="token plain"> </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-i</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">'s/127.0.0.1/10.10.10.100/'</span><span class="token plain"> k3s.yaml</span><br></span></code></pre></div></div>
<p>No we can manage the cluster from our local machine:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token builtin class-name" style="color:rgb(255, 203, 107)">export</span><span class="token plain"> </span><span class="token assign-left variable" style="color:rgb(191, 199, 213)">KUBECONFIG</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token variable" style="color:rgb(191, 199, 213)">$(</span><span class="token variable builtin class-name" style="color:rgb(255, 203, 107)">pwd</span><span class="token variable" style="color:rgb(191, 199, 213)">)</span><span class="token plain">/k3s.yaml</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">kubectl get nodes</span><br></span></code></pre></div></div>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="upgrade-plan-️">Upgrade Plan 🏗️📝📐<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#upgrade-plan-%EF%B8%8F" class="hash-link" aria-label="Upgrade Plan 🏗️📝📐 への直接リンク" title="Upgrade Plan 🏗️📝📐 への直接リンク" translate="no">​</a></h2>
<p>The plan I'm using will keep k3s updated to the latest patch version of the channel we give. In this case I'm using the <code>v1.31</code> channel, the same channel used above. Kubernetes v1.31.4 just released at time of writing this post, so with this plan we have stable upgrades handled for the next 10-12 months (depending on how many patch releases this minor gets).</p>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="install-the-system-upgrade-controller">Install the system-upgrade-controller<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#install-the-system-upgrade-controller" class="hash-link" aria-label="Install the system-upgrade-controller への直接リンク" title="Install the system-upgrade-controller への直接リンク" translate="no">​</a></h3>
<p>The upgrade plan is managed by the system-upgrade-controller. Install it:</p>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-f</span><span class="token plain"> https://github.com/rancher/system-upgrade-controller/releases/latest/download/system-upgrade-controller.yaml</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-f</span><span class="token plain"> https://github.com/rancher/system-upgrade-controller/releases/latest/download/crd.yaml</span><br></span></code></pre></div></div>
<h3 class="anchor anchorTargetStickyNavbar_Vzrq" id="create-the-upgrade-plan">Create the upgrade plan<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#create-the-upgrade-plan" class="hash-link" aria-label="Create the upgrade plan への直接リンク" title="Create the upgrade plan への直接リンク" translate="no">​</a></h3>
<div class="language-yaml codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-yaml codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#server-plan.yaml</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">apiVersion</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> upgrade.cattle.io/v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">kind</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Plan</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">metadata</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">name</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> server</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">plan</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">namespace</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> system</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">upgrade</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token key atrule">spec</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">concurrency</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">cordon</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token boolean important" style="color:rgb(255, 88, 116)">true</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">nodeSelector</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">matchExpressions</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain"> </span><span class="token key atrule">key</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> node</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">role.kubernetes.io/control</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">plane</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      </span><span class="token key atrule">operator</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> In</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      </span><span class="token key atrule">values</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      </span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">"true"</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">serviceAccountName</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> system</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">upgrade</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">upgrade</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token key atrule">image</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> rancher/k3s</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">upgrade</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><span class="token key atrule">channel</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> https</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain">//update.k3s.io/v1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-</span><span class="token plain">release/channels/v1.31</span><br></span></code></pre></div></div>
<div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">kubectl apply </span><span class="token parameter variable" style="color:rgb(191, 199, 213)">-f</span><span class="token plain"> server-plan.yaml</span><br></span></code></pre></div></div>
<p>See the <a class="" href="https://docs.k3s.io/ja/upgrades/automated">automated upgrade docs</a> for more details.</p>
<h2 class="anchor anchorTargetStickyNavbar_Vzrq" id="conclusion-">Conclusion 🚀<a href="https://docs.k3s.io/ja/blog/2025/03/10/simple-ha#conclusion-" class="hash-link" aria-label="Conclusion 🚀 への直接リンク" title="Conclusion 🚀 への直接リンク" translate="no">​</a></h2>
<p><img decoding="async" loading="lazy" alt="kubectl summary" src="https://docs.k3s.io/ja/assets/images/kubectl-e58868310be82a01e6ad9e0fbc4a0e32.png" width="1089" height="424" class="img_ev3q"></p>
<p>We now have a high-availability cluster, accessible via a single IP. Upgrades are handled for the next year. This is a great starting point to:</p>
<ul>
<li class="">Add agent nodes to expand our workload capacity</li>
<li class="">Add another load-balancer for additional redundancy</li>
</ul>]]></content>
        <author>
            <name>Derek Nola</name>
            <uri>https://github.com/dereknola</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Blog]]></title>
        <id>https://docs.k3s.io/ja/blog/2025/03/09/hello-blog</id>
        <link href="https://docs.k3s.io/ja/blog/2025/03/09/hello-blog"/>
        <updated>2025-03-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This is the first blog post on k3s.io]]></summary>
        <content type="html"><![CDATA[<p>This is the first post on blog.k3s.io</p>
<p>We will explore aspects of K3s, Kubernetes, and other related topics. These long form posts will be written by the K3s team and help illuminate aspects of the project that are not easily covered in the documentation.</p>
<p>Stay tuned for more posts in the future.</p>]]></content>
        <author>
            <name>Derek Nola</name>
            <uri>https://github.com/dereknola</uri>
        </author>
    </entry>
</feed>