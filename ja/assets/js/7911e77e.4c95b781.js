"use strict";(self.webpackChunkk_3_s_docs=self.webpackChunkk_3_s_docs||[]).push([[9081],{7677:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2026/01/15/K3s-1.35-release","metadata":{"permalink":"/ja/blog/2026/01/15/K3s-1.35-release","source":"@site/blog/2026-01-15-K3s-1.35-release.md","title":"Kubernetes v1.35 is out!","description":"A deep dive into the latest K3s features, security enhancements, and community milestones","date":"2026-01-15T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/manuelbuil","name":"Manuel Buil","imageURL":"https://github.com/manuelbuil.png","key":"manuelbuil","page":null}],"frontMatter":{"title":"Kubernetes v1.35 is out!","description":"A deep dive into the latest K3s features, security enhancements, and community milestones","authors":"manuelbuil","hide_table_of_contents":true},"unlisted":false,"nextItem":{"title":"K3s strategies for image consumption","permalink":"/ja/blog/2025/11/11/strategies-for-large-images"}},"content":"Kubernetes 1.35 has arrived! As we roll out this latest version, it\u2019s the perfect time to reflect on the significant strides the K3s project has made over the last quarter. \ud83e\udd73\\n\\n\x3c!-- truncate --\x3e\\n\\nOur focus this release was on stability. We know that our users value a platform that remains consistent and reliable year over year. From major security milestones to core language upgrades, here is what\u2019s new in the world of K3s.\\n\\n## Key Features and Improvements \u2728\\n\\n#### Modular Executor Interface & CNI Startup \ud83c\udfd7\ufe0f\\nOne of the most significant architectural improvements in this release is the refactoring of the Executor interface. By making CNI startup a first-class part of this interface, we have removed reliance on \\"CLI flag hacks\\" and direct dependencies on networking providers like Flannel or Kube-router. This decoupling makes K3s more modular and significantly easier to integrate into diverse platforms and distributions.\\n\\n#### Embedded Kine Metrics \ud83d\udcca\\nVisibility into your database layer just got a lot better. We\u2019ve enabled embedded metrics for Kine, allowing you to monitor the performance of your external database (like PostgreSQL or MySQL) directly through the K3s metrics endpoint. This is essential for debugging latency at the storage layer before it impacts your workloads.\\n\\n#### Node Password Secrets \ud83d\udd10\\nSecurity and automation take a step forward with the evolution of the Node Password Secrets. Instead of using the generic `opaque` type, we use a custom type. This allows the node password secret controller to only watch node password secrets instead of all secrets, which improves resource utilization and increases responsiveness in larger clusters. Node password secrets are also now automatically cleaned up if node registration fails, instead of leaving orphaned secrets.\\n\\n#### Security Self-Assessment \ud83d\udee1\ufe0f\\nAs part of our commitment to excellence, we have completed a comprehensive Security Self-Assessment. This deep dive into our architecture and processes ensures that K3s continues to meet the highest standards for production environments, providing a clear roadmap for how we protect your data. [link](https://github.com/cncf/toc/pull/1986)\\n\\n#### Language Evolution: Go 1.25 \ud83d\ude80\\nK3s is now built with **Go 1.25**. This brings the latest performance optimizations and security patches to the K3s binary, ensuring that our \\"default\\" remains the most efficient way to run Kubernetes.\\n\\n#### Inclusive Naming Migration \ud83e\udd1d\\nWords matter. We have officially transitioned our primary branch naming from `master` to `main` and updated internal references to align with [inclusive naming standards](https://www.cncf.io/announcements/2021/10/13/inclusive-naming-initiative-announces-new-community-resources-for-a-more-inclusive-future/). This ensures our codebase remains welcoming to all developers who wish to contribute.\\n\\nYou can find more information on what v1.35 brought upstream in this cool [blog post](https://kubernetes.io/blog/2025/12/17/kubernetes-v1-35-release/)\\n\\n## Bug Fixes and Notable Changes \ud83d\udee0\ufe0f\\n\\nAs always, there were a lot of bug fixes and here, are the most impactful:\\n\\n* **Networking & Dual-Stack Resilience:** We\'ve pushed several fixes to stabilize complex networking setups. This includes corrected IPv6 handling for LoadBalancer addresses, resolving fatal errors in Network Policies (NetPol) when node IPs change, and hardening our Tailscale integration to handle pre-existing configurations more gracefully.\\n* **Reliable HA Cluster Management:** Resolved critical edge-case bugs related to etcd member promotion and bootstrap data reconciliation. This ensures that when etcd members join or leave, or are restarted, the quorum remains stable and the promotion process is seamless.\\n* **CI Pipeline Migration:** We successfully migrated the K3s Pull Request and Release CI pipelines from Drone to GitHub Actions, increasing transparency and ensuring the continuity of our build process for years to come.\\n\\n## Version Bumps for Key Components \ud83d\ude80\\n\\n| Component | New Version |\\n| :--- | :--- |\\n| Kine | v0.14.9 |\\n| SQLite | v3.50.4 |\\n| Etcd | v3.6.6 |\\n| Containerd | v2.1.5 |\\n| Runc | v1.4.0 |\\n| Flannel | v0.27.4 |\\n| Metrics-server | v0.8.0 |\\n| Traefik | v3.5.1 |\\n| Coredns | v1.13.1 |\\n| Helm-controller | v0.16.17 |\\n| Local-path-provisioner | v0.0.32 |\\n\\n## Special Thanks to Our Contributors \ud83d\ude4f\\n\\nWe are incredibly grateful to our community members who contributed key improvements during this cycle. A massive thank you to:\\n\\n**@systemj**, **@farazkhawaja**, **@AshiqN**, **@rorosen**, **@xelus22**, **@jvassev**\\n\\n## Join our Adopters list \ud83d\udc8e\\n\\nIf K3s is making your life easier, the best way to say \\"thanks\\" is to add your company to our official Adopters list. It\u2019s a tiny gesture that carries a lot of weight for the project\'s health and visibility within the CNCF ecosystem. We are currently working hard to get our \'status\' inside the CNCF to progress and showing a large list of Adopters would help tremendously.\\n\\nThe task is easy: create a PR that adds your name in https://github.com/k3s-io/k3s/blob/main/ADOPTERS.md.\\n\\nThanks a lot!"},{"id":"/2025/11/11/strategies-for-large-images","metadata":{"permalink":"/ja/blog/2025/11/11/strategies-for-large-images","source":"@site/blog/2025-11-11-strategies-for-large-images.md","title":"K3s strategies for image consumption","description":"Master online and offline image loading techniques in K3s for ultra-fast application startup, even with multi-gigabyte containers.","date":"2025-11-11T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/manuelbuil","name":"Manuel Buil","imageURL":"https://github.com/manuelbuil.png","key":"manuelbuil","page":null}],"frontMatter":{"title":"K3s strategies for image consumption","description":"Master online and offline image loading techniques in K3s for ultra-fast application startup, even with multi-gigabyte containers.","authors":"manuelbuil","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Kubernetes v1.35 is out!","permalink":"/ja/blog/2026/01/15/K3s-1.35-release"},"nextItem":{"title":"Sysbox Runtime With K3s","permalink":"/ja/blog/2025/09/27/k3s-sysbox"}},"content":"Slow image pulls can be annoying and may increase Kubernetes startup times over a healthy threshold, particularly in resource-constrained or air-gapped environments. The situation is exacerbated by new AI-driven apps, which often rely on astronomically large images, frequently tens or hundreds of gigabytes. This post dives into mechanisms that K3s makes available to improve the user\'s experience when handling large images.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Online & Offline Strategies: The Power of Local Import \ud83d\udce6 ##\\n\\nK3s provides mechanisms for ensuring large images are available quickly, that address two common scenarios:\\n- Online Clusters: To avoid slow image pulls from an external registry when a pod starts, K3s can `pre-pull` images from a manifest file.\\n- Offline (Air-Gapped) Clusters: Where no external registry is available, K3s can `import` images directly from local tarball archives.\\n\\n1. Pre-Pulling Images via a Manifest File (Online)\\nIn scenarios with internet connectivity, the goal is to initiate image pulls as early and efficiently as possible. K3s can be instructed to sequentially pull a set of images into the embedded containerd store during startup or while K3s is running. This is ideal for ensuring base images are ready the moment the cluster starts or the moment the application is deployed. However, if this process is done before the cluster is started, K3s won\'t successfully start until all images have been pulled, which could make K3s fail to start if it takes more than 15 minutes. If you suspect this is happening to you, you\'d better do the pre-pulling while K3s is running. \\n\\nUsers can trigger a pull of images into the containerd image store by placing a simple text file containing the image names, one per line, in the `/var/lib/rancher/k3s/agent/images` directory. As we have just explained, this can be done before K3s starts or while K3s is running. For example, you can execute the following in one of the nodes:\\n\\n```bash\\nmkdir -p /var/lib/rancher/k3s/agent/images && echo docker.io/pytorch/pytorch:2.9.0-cuda12.6-cudnn9-runtime > /var/lib/rancher/k3s/agent/images/pytorch.txt\\n```\\nIn the previous command, we have created the images directory on the node and dropped a file names `pytorch.txt` that contains the image: `docker.io/pytorch/pytorch:2.9.0-cuda12.6-cudnn9-runtime`.\\n\\nThe K3s process will then pull these images via the CRI API. You should see the following two logs:\\n```log\\n# When the k3s controller detects the file\\nlevel=info msg=\\"Pulling images from /var/lib/rancher/k3s/agent/images/example.txt\\"\\nlevel=info msg=\\"Pulling image docker.io/pytorch/pytorch:2.9.0-cuda12.6-cudnn9-runtime\\"\\n\\n# When the import is ready. It specifies how much time it took in ms:\\nlevel=info msg=\\"Imported docker.io/pytorch/pytorch:2.9.0-cuda12.6-cudnn9-runtime\\"\\nlevel=info msg=\\"Imported images from /var/lib/rancher/k3s/agent/images/example.txt in 6m1.178972902s\\"\\n```\\n\\n2. Importing Images from Tarballs (Offline & Ultra-Fast)\\n\\nFor the absolute fastest startup\u2014critical or when being in an air-gapped environment, the images should be available locally as tarballs. K3s will load these images directly into the containerd image store, bypassing any network traffic entirely.\\n\\nPlace the image tarballs (created using docker save or ctr save) in the same /var/lib/rancher/k3s/agent/images directory. K3s will decompress the tarball, extract the image layers, and load them.\\n\\nFor example, I have created an image tarball with all the images required to deploy the popular [microservices-demo](https://github.com/GoogleCloudPlatform/microservices-demo) with the name `microservices-demo.tar.gz`.\\n\\n```bash\\n# Example: Save the image and place the tarball\\nmkdir -p /var/lib/rancher/k3s/agent/images\\ncp microservices-demo.tar.gz /var/lib/rancher/k3s/agent/images/\\n```\\n\\nThe K3s process will load those images and you should see the following two logs:\\n```log\\nlevel=info msg=\\"Importing images from /var/lib/rancher/k3s/agent/images/microservices-demo.tar.gz\\"\\nlevel=info msg=\\"Imported images from /var/lib/rancher/k3s/agent/images/microservices-demo.tar.gz in 1m39.8610592s\\n```\\n\\nYou can verify the successfully imported images at any time using the bundled client: `k3s ctr images list`\\n\\n### Optimizing booting times with tarballs ###\\n\\nBy default, image archives are imported every time K3s starts to ensure consistency. However, this delay can be significant when dealing with many large archives, for example, `microservices-demo.tar.gz` took 1m39s to import. To alleviate this, K3s offers a feature to only import tarballs that have changed since they were last processed. To enable this feature, create an empty `.cache.json` file in the images directory:\\n\\n```bash\\ntouch /var/lib/rancher/k3s/agent/images/.cache.json\\n```\\n\\nThe cache file will store archive metadata (size and modification time). Subsequent restarts of K3s will check this file and skip the import process for any large tarballs that haven\'t changed, dramatically speeding up cluster boot time. Therefore, to check that this is working, check `.cache.json` is not empty and, after restarting, that the two log lines do not appear anymore.\\n\\nNote that the caching mechanism needs to be enabled carefully. If an image was removed or pruned since last startup, take manual action to reimport the image. Check our [docs](https://docs.k3s.io/installation/airgap?_highlight=.cache.json&airgap-load-images=Manually+Deploy+Images#enable-conditional-image-imports) for more information.\\n\\n\\n## Embedded Registry Mirror \ud83d\udd78\ufe0f ##\\n\\nK3s offers an in-cluster container image registry mirror by embedding [Spegel](https://spegel.dev/). Its primary use case is to accelerate image pulling and reduce external network dependency in Kubernetes clusters by allowing nodes to pull cached image content directly from other nodes whenever possible, instead of requiring each node to reach out to a central registry. To enable this feature, server nodes must be started with the `--embedded-registry` flag, or with `embedded-registry: true` in the configuration file. When enabled, every node in your cluster instantly becomes a stateless, local image mirror listening on port 6443. Nodes share a constantly updated list of available images over a peer-to-peer network on port 5001.\\n\\n```bash\\n# Enable the embedded registry mirror\\nembedded-registry: true\\n# To enable metrics that can help with the embedded registry mirror\\nsupervisor-metrics: true \\n```\\n\\nAnd then, on all nodes, you must add a `registries.yaml` where we specified what registries to allow a node to both push and pull images with other nodes. If a registry is enabled for mirroring on some nodes, but not on others, only the nodes with the registry enabled will exchange images. For example:\\n\\n```yaml\\nmirrors:\\n  docker.io:\\n  registry.k8s.io:\\n```\\n\\nIf everything boots up correctly, you should see in the logs:\\n```log\\nlevel=info msg=\\"Starting distributed registry mirror at https://10.11.0.11:6443/v2 for registries [docker.io registry.k8s.io]\\"\\nlevel=info msg=\\"Starting distributed registry P2P node at 10.11.0.11:5001\\"\\n```\\n\\nAnd you should be able to see metrics of Spegel by querying the supervisor metrics server:\\n```bash\\nkubectl get --server https://10.11.0.11:6443 --raw /metrics  | grep spegel\\n```\\n\\nFor more information check the [docs](https://docs.k3s.io/installation/registry-mirror)\\n\\n## Bonus: eStargz images \u26a1 ##\\n\\nA different solution to speed up the creation of pods is by using a special image format called eStargz. This enables lazy pulling, which means that the application can start almost instantly while the rest of the image is pulled in the background. This strategy requires both the image to be specifically built in the eStargz format and the K3s agent to be configured to use the stargz snapshotter: `--snapshotter=estargz` flag, or with `snapshotter: estargz` in the configuration file.\\n\\nThis is currently an experimental feature in K3s and we have more information in the [advanced section of our docs](https://docs.k3s.io/advanced#enabling-lazy-pulling-of-estargz-experimental). We would love to hear your feedback if you are using it.\\n\\n## Conclusion \ud83c\udfc1 ##\\n\\nK3s provides robust, flexible tools to tackle slow image pulls, a problem magnified by today\'s multi-gigabyte cloud-native and AI images. By leveraging pre-pulling manifest strategies, tarball loading or optimizing image distribution with the embedded [Spegel](https://spegel.dev/) registry mirror, you can shift slow network operations into quick local operations. These mechanisms ensure your resource-constrained and air-gapped clusters achieve rapid, predictable startup times, delivering a consistently better user experience."},{"id":"/2025/09/27/k3s-sysbox","metadata":{"permalink":"/ja/blog/2025/09/27/k3s-sysbox","source":"@site/blog/2025-09-27-k3s-sysbox.md","title":"Sysbox Runtime With K3s","description":"Integrating sysbox runtime with k3s\' containerd","date":"2025-09-27T00:00:00.000Z","tags":[{"inline":true,"label":"runc","permalink":"/ja/blog/tags/runc"},{"inline":true,"label":"sysbox","permalink":"/ja/blog/tags/sysbox"}],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/galal-hussein","name":"Hussein Galal","imageURL":"https://github.com/galal-hussein.png","key":"husseingalal","page":null}],"frontMatter":{"title":"Sysbox Runtime With K3s","description":"Integrating sysbox runtime with k3s\' containerd","authors":"husseingalal","tags":["runc","sysbox"],"hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"K3s strategies for image consumption","permalink":"/ja/blog/2025/11/11/strategies-for-large-images"},"nextItem":{"title":"Kubernetes v1.34 is out!","permalink":"/ja/blog/2025/08/30/K3s-1.34-release"}},"content":"The K3s binary bundles all the components needed to run a production-ready, CNCF-conformant Kubernetes cluster including containerd, runc, kubelet, and more. In this post we will discuss how containerd communicates with OCI runtimes and will discuss adding another container runtime (Sysbox) to K3s and how it can be used to run system pods in your environment.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Containerd and Runc\\n\\nFirst we need to talk briefly about how containerd works with runc. Containerd is a long running daemon that is responsible for:\\n\\n- **Image Management**: Pulls and stores images from registries.\\n- **Container Management**: Manages the lifecycle of containers (create, start, stop, delete).\\n- **Snapshot Management**: Uses snapshotters to manage the filesystem layers for containers.\\n- **Runtime Management**: Delegates the creation of containers to OCI-compatible runtimes like runc.\\n\\nWhen you create a pod in Kubernetes, kubelet uses the CRI plugin implemented in containerd to request a **pod sandbox** (`RunPodSandbox`) and then container creation (`CreateContainer`). Containerd then calls a `shim` process that acts as a middleman between `containerd` and the OCI runtime (for example `runc`). This shim process allows the container to keep running even if the containerd daemon crashes or restarts.  \\n\\nThe shim generates the OCI runtime bundle (`config.json` and `rootfs` path) and then executes the runc binary. Runc reads the `config.json`, sets up the container\u2019s namespaces and cgroups, and then launches the container process.  \\n\\nRunc is the component that directly interfaces with the Linux kernel \u2014 configuring cgroups, namespaces, seccomp, capabilities, and mounts. After runc finishes creating the container, it **exits**, leaving the shim to manage the lifecycle and I/O of the container.\\n\\n```mermaid\\ngraph TD\\n    Kubelet[\\"Kubelet<br/>(CRI)\\"]\\n    Containerd[\\"Containerd\\"]\\n    Shim[\\"Shim\\"]\\n    Runtime[\\"OCI Runtime<br/>(runc)\\"]\\n    Container[\\"Container\\"]\\n\\n    Kubelet --\x3e|\\"RunPodSandbox()\\n    CreateContainer()\\"| Containerd\\n    Containerd --\x3e|\\"Start/Stop Task\\"| Shim\\n    Shim --\x3e|\\"Executes Container\\"| Runtime\\n    Runtime --\x3e|\\"clone()\\n    cgroupns()\\n    setns()\\"| Container\\n```\\n\\n## Sysbox Runtime\\n\\n[Sysbox](https://github.com/nestybox/sysbox) is an open-source, next-generation container runtime created by Nestybox. Unlike traditional runtimes (such as runc), Sysbox is designed to let you run \\"system containers\\". It primarily leverages **Linux user namespaces** and other features to provide containers that behave more like lightweight virtual machines.\\n\\nThis means you can run workloads like Docker, Systemd, containerd, or even K3s inside your pods \u2014 all without requiring privileged mode.\\n\\nIn short, Sysbox bridges the gap between application containers and virtual machines, enabling use cases like running Kubernetes-in-Kubernetes (K8s-in-K8s), CI/CD pipelines that need full OS-like environments, or development sandboxes with VM-level isolation but container speed.\\n\\n:::info\\nCurrently, Sysbox officially supports **CRI-O** only. CRI-O has native support for Linux user namespaces, which Sysbox relies on. While containerd added user namespace support starting in version v2.0, there was a [bug](https://github.com/nestybox/sysbox/issues/958) in sysbox-runc that prevented it from working properly with Sysbox.\\n:::\\n\\n## Sysbox-runc Containerd Integration\\n\\nAfter investigating this issue, I was able to locate the root cause, as explained in this [PR](https://github.com/nestybox/sysbox-runc/pull/106). Containerd was failing to run a specific subcommand for sysbox-runc called `features`, which led to the following error:\\n\\n```\\nlevel=debug msg=\\"failed to introspect features of runtime \\\\\\"sysbox-runc\\\\\\"\\" error=\\"failed to unmarshal Features (*anypb.Any): type with url : not found\\"\\n```\\n\\nBecause of this, containerd instructed sysbox-runc to run containers **without user namespaces**, causing container creation to fail. The fix for this bug was recently merged in the `sysbox-runc` repo, enabling containerd to work with sysbox-runc.\\n\\n# Running Sysbox-runc With K3S\\n\\nIn order to run `sysbox-runc` with K3s you need to have a running K3s cluster, and then you can proceed to install the latest version of sysbox, However, since the fix for containerd support hasn\'t yet been integrated to sysbox main repo only in `sysbox-runc`, we need to build the binaries from source to get the latest updates.\\n\\n1. Install docker in your system.\\n\\n2. Clone the repo and prepare the code\\n\\n```\\ngit clone --recursive https://github.com/nestybox/sysbox.git\\n\\ncd sysbox/sysbox-runc\\n\\ngit pull origin main\\n\\ncd ..\\n\\nmake IMAGE_BASE_DISTRO=ubuntu IMAGE_BASE_RELEASE=jammy sysbox-static\\n```\\n\\nYou can then copy the binaries built to `/usr/bin` or if you are building on the same machine that you will run containerd you can just run:\\n\\n```\\nmake install\\n```\\n\\n3. Run sysbox binary\\n\\n```\\nsysbox\\n```\\n\\n4. Create sysbox runc runtime class\\n\\n```\\napiVersion: node.k8s.io/v1\\nhandler: sysbox-runc\\nkind: RuntimeClass\\nmetadata:\\n  name: sysbox-runc\\n```\\n\\n5. Add sysbox runc to containerd configuration, you can do that by creating `/var/lib/rancher/k3s/agent/etc/containerd/config.toml.tmpl`:\\n\\n```\\n[plugins.\'io.containerd.cri.v1.runtime\'.containerd.runtimes.sysbox-runc]\\n  runtime_type = \\"io.containerd.runc.v2\\"\\n\\n[plugins.\'io.containerd.cri.v1.runtime\'.containerd.runtimes.sysbox-runc.options]\\n  SystemdCgroup = false\\n  BinaryName=\\"/usr/bin/sysbox-runc\\"\\n```\\n\\n6. Finally you can create pod running with the runtime class for sysbox-runc and `hostUsers: false`:\\n\\n```\\napiVersion: v1\\nkind: Pod\\nmetadata:\\n  name: ubuntu\\nspec:\\n  runtimeClassName: sysbox-runc\\n  hostUsers: false\\n  containers:\\n  - name: ubuntu2204\\n    image: ubuntu:22.04\\n    command: [\\"sleep\\", \\"40000000000\\"]\\n  restartPolicy: Never\\n```\\n\\n## Conclusion\\n\\nSysbox brings a powerful capability to Kubernetes: the ability to run system-level workloads inside containers with strong isolation, without requiring privileged mode. When combined with K3s, this opens the door to new use cases such as:\\n\\n- Running Kubernetes-in-Kubernetes clusters for virtual clusters ([k3k](https://github.com/rancher/k3k)).\\n- Creating secure developer sandboxes that behave like lightweight VMs.  \\n- Running system daemons or nested container engines inside pods.  \\n\\nWhile Sysbox is officially supported with CRI-O today, the recent fixes in `sysbox-runc` allow it to run on containerd as well \u2014 making it possible to integrate with K3s. The integration is still evolving, but it shows how the container ecosystem is moving beyond traditional app containers toward more flexible \\"system containers.\\"\\n\\nIf you\u2019re experimenting with K3s and want to explore system workloads inside pods, Sysbox provides a compelling way to do so while maintaining Kubernetes-native workflows"},{"id":"/2025/08/30/K3s-1.34-release","metadata":{"permalink":"/ja/blog/2025/08/30/K3s-1.34-release","source":"@site/blog/2025-08-30-K3s-1.34-release.md","title":"Kubernetes v1.34 is out!","description":"Step back and look at K3s recent work and achievements","date":"2025-08-30T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/manuelbuil","name":"Manuel Buil","imageURL":"https://github.com/manuelbuil.png","key":"manuelbuil","page":null}],"frontMatter":{"title":"Kubernetes v1.34 is out!","description":"Step back and look at K3s recent work and achievements","authors":"manuelbuil","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Sysbox Runtime With K3s","permalink":"/ja/blog/2025/09/27/k3s-sysbox"},"nextItem":{"title":"K3s initialization deep dive","permalink":"/ja/blog/2025/03/25/K3s-initialization"}},"content":"Kubernetes 1.34 is finally here! A look back at K3s recent work and achievements \ud83e\udd73\\n\\nIt\'s been a busy and productive few months, and we\'re excited to share some of the amazing progress we\'ve made. From new features that make your lives easier to important bug fixes and foundational improvements, our team and community have been hard at work.\\n\\n\x3c!-- truncate --\x3e\\n\\n## Key Features and Improvements \u2728\\nHere\u2019s a look at some of the most impactful features and updates we\'ve rolled out:\\n\\n#### Automatic Certificate Renewal Window Increase \ud83d\udcc5\\nWe\'ve made certificate management more user-friendly by increasing the automatic certificate renewal window from 90 to 120 days. This means if you perform quarterly upgrades, your certificates will be renewed more frequently, preventing them from expiring and making your clusters unusable. For more information check our [docs](https://docs.k3s.io/cli/certificate#client-and-server-certificates)\\n\\n#### Optional Airgap Image Tarball Imports \ud83d\udca8\\nFor those who use K3s in an airgap environment, you now have the option to skip importing all image tarballs. You can select to import only images that have changed since they were last imported, even across restarts. This can significantly speed up the startup process when deploying a large number of images, as the kubelet can start sooner. Check our [docs](https://docs.k3s.io/installation/airgap?airgap-load-images=Manually+Deploy+Images#enable-conditional-image-imports) for more information\\n\\n#### Enhanced Certificate Check Output \u2705\\nWe\'ve improved the output of our certificate checks to provide clearer information about certificate usage and expiration dates. The output can also now be viewed in different formats. Our [docs](https://docs.k3s.io/cli/certificate#checking-expiration-dates) show an example of the new output format.\\n\\n#### Kube-scheduler and Kube-controller-manager Certificate Management \ud83d\udd10\\nWe are now generating and managing certificates for kube-scheduler and kube-controller-manager, and they can be rotated using our existing certificate rotation [tool](https://docs.k3s.io/cli/certificate).\\n\\n#### Retention Flag for S3 Stored Snapshots \ud83d\udcbe\\nWe\'ve added a new retention flag for snapshots stored in an S3 bucket. This allows you to keep snapshots in S3 for longer periods while maintaining a smaller number of local snapshots. This is a great feature for balancing long-term disaster recovery with local storage efficiency. Check out our [docs](https://docs.k3s.io/cli/etcd-snapshot#s3-retention) for more information about how to use it.\\n\\n#### Official Governance Model \ud83e\udd1d\\nWe\'re proud to announce that we now have an official governance model in place! This will help bring more clarity to our project and hopefully encourage more developers to join our fantastic community. You can read it [here](https://github.com/k3s-io/k3s/blob/master/GOVERNANCE.md).\\n\\n## Bug Fixes and Other Notable Changes \ud83d\udee0\ufe0f\\nWe\'ve also been busy tackling a number of bugs, tech debt items and making other important improvements under the hood:\\n\\nNumerous bug fixes, including those for secrets encryption timeouts and race conditions, DNS fallbacks, various authorization and authentication handling issues and replacing go-bindata with the go native embed package.\\n\\nImprovements to our test and build infrastructure, e.g. migrating of K3s release artifacts to GitHub Actions (GHA).\\n\\n## Version bumps for key components \ud83d\ude80\\n\\nApart from Kubernetes v1.34.x, we bumped versions for several key components. Here is the list with the latest versions:\\n\\n| Component | New Version |\\n| :--- | :--- |\\n| Kine | v0.14.0 |\\n| SQLite | v3.50.4 |\\n| Etcd | v3.6.4 |\\n| Containerd | v2.1.4 |\\n| Runc | v1.3.1 |\\n| Flannel | v0.27.0 |\\n| Metrics-server | v0.8.0 |\\n| Traefik | v3.3.6 |\\n| Coredns | v1.12.3 |\\n| Helm-controller | v0.16.13 |\\n| Local-path-provisioner | v0.0.32 |\\n\\n\\n## Special Thanks to Our Contributors \ud83d\ude4f\\nWe want to give a special shout-out to the incredible contributors who are not part of our core maintainers list. Your work is invaluable to the project\'s success. Thank you to: @ErikJiang, @eggplants, @muicoder, @eugercek, @yulken, @l2dy, @OrlinVasilev\\n\\nWe look forward to an even more productive future with all of you!"},{"id":"/2025/03/25/K3s-initialization","metadata":{"permalink":"/ja/blog/2025/03/25/K3s-initialization","source":"@site/blog/2025-03-25-K3s-initialization.md","title":"K3s initialization deep dive","description":"Explain k3s initialization steps","date":"2025-03-25T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/manuelbuil","name":"Manuel Buil","imageURL":"https://github.com/manuelbuil.png","key":"manuelbuil","page":null}],"frontMatter":{"title":"K3s initialization deep dive","description":"Explain k3s initialization steps","authors":"manuelbuil","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Kubernetes v1.34 is out!","permalink":"/ja/blog/2025/08/30/K3s-1.34-release"},"nextItem":{"title":"The Basic HA Cluster","permalink":"/ja/blog/2025/03/10/simple-ha"}},"content":"K3s is a lightweight Kubernetes distribution which excels in its deployment speed and minimal resource footprint. In fact, a lot of our users love K3s because it offers an unparalleled initialization speed.\\n\\n\x3c!-- truncate --\x3e\\nThis blog post delves into the heart of K3s\'s efficiency: its initialization process. We\'ll embark on a journey through the steps that enable K3s to materialize a fully functional Kubernetes cluster so quickly. By examining K3s\u2019s own logs, we\'ll unravel the meaning behind each step, providing you with a practical understanding of how K3s achieves its remarkable speed. This exploration not only illuminates the inner workings of K3s but also equips you with the knowledge to troubleshoot your deployments.\\n\\n## The Embedded Powerhouse \u2699\ufe0f\u26a1\\nK3s leverages `go-bindata` to embed essential Linux userspace binaries and manifests directly into its executable. This eliminates external dependencies and streamlines the deployment process. Within the K3s binary, you\'ll find core components like `runc` and `containerd`, along with the k3s-root tarball (e.g. k3s-root-amd64.tar). This tarball contains all the userspace binaries necessary for K3s to function, reducing the reliance on the host OS. If you would like all the K3s embedded binaries to take preference over the host OS binaries, you should use the `--prefer-bundled-bin` flag.\\n\\nThe embedded binaries are always deployed in the same directory: `/var/lib/rancher/k3s/data`. If you inspect this folder, you will notice it contains at least three subdirectories: `cni`, `current` and a long string of characters (or SHA). That long string of characters is generated when building K3s and it is the result of a `sha256sum` operation made on the tarball with the embedded binaries. As these change in each release, you will see a different string of characters for each release. In fact, after an upgrade, there will be two directories with a long string of characters as their name.\\n\\n`current` is just a symlink to the SHA directory and `cni` includes different cni plugins that are also symlinks to the cni binary in the SHA directory. This is because we are building all cni plugins in just one binary using multi-exec tooling. This is again a way to be more efficient and less resource consuming. If your current K3s deployment underwent an upgrade process, you will see one extra directory called `previous`, which is another symlink to the previous SHA directory. For clarification this example:\\n\\n```\\n$> ls -ahltr /var/lib/rancher/k3s/data/\\ntotal 24K\\n-rw------- 1 root root    0 Mar 19 06:22 .lock\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:22 ..\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:28 82142f5157c67effc219aeefe0bc03e0460fc62b9fbae9e901270c86b5635d53\\nlrwxrwxrwx 1 root root   90 Mar 19 06:28 previous -> /var/lib/rancher/k3s/data/82142f5157c67effc219aeefe0bc03e0460fc62b9fbae9e901270c86b5635d53\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:30 b13851fe661ab93938fc9a881cdce529da8c6b9b310b2440ef01a860f8b9c3a9\\nlrwxrwxrwx 1 root root   90 Mar 19 06:30 current -> /var/lib/rancher/k3s/data/b13851fe661ab93938fc9a881cdce529da8c6b9b310b2440ef01a860f8b9c3a9\\ndrwxr-xr-x 2 root root 4.0K Mar 19 06:30 cni\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:40 .\\n```\\n\\nAdditionally, K3s includes embedded Helm charts and manifests for deploying critical services such as CoreDNS, Traefik, and local storage. These embedded charts, formatted as yaml files, can be found in the control plane nodes in the directory: `/var/lib/rancher/k3s/server/manifests`.\\n\\nFor 67MB our K3s binary includes a lot of stuff!\\n\\n\\n## The Boot Sequence, Step-by-Step \ud83d\udc63\\nNow that we established how K3s is carrying its embedded tools, we can explore the boot up sequence. Let us look at the typical logs you can find in `journalctl` when deploying a control-plane or K3s server instance. It all starts with:\\n\\n```\\nStarting Lightweight Kubernetes...\\n```\\n\\nAnd then:\\n\\n```\\n/usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service\\n```\\n\\nThis part is checking for a network manager utility which must be disabled as described in the [docs](https://docs.k3s.io/installation/requirements?_highlight=nm&_highlight=cloud&_highlight=setup.service&os=rhel#operating-systems). It configures some parts of the network stack, specifically the routing tables, which conflict with Kubernetes networking, and that is why we verify if it was correctly disabled.\\n\\nThe next message should look familiar\\n\\n```\\nAcquiring lock file /var/lib/rancher/k3s/data/.lock\\nPreparing data dir /var/lib/rancher/k3s/data/f8e9b5e7d85085972f4a9ddfd539d4dcf887be2e380a55f415c93cac5516dad5\\n```\\n\\nWhen this message is shown, the directory where K3s deploys the embedded binaries has already been created. At this point, K3s will extract the binaries. We use the lock to avoid concurrent modifications, preventing K3s embedded commands like `kubectl` or `ctr` from executing and disturbing the K3s initialization.\\n\\nThe next block of logs point at the K3s version and the datastore. In this case, I am using the default datastore which means kine with sqlite. For more information on the different datastores available check this [link](https://docs.k3s.io/datastore)\\n\\n```\\nStarting k3s v1.32.2+k3s1\\nConfiguring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s\\nConfiguring database table schema and indexes, this may take a moment...\\nDatabase tables and indexes are up to date\\nKine available at unix://kine.sock\\n```\\n\\nOnce the datastore is available k3s locks the bootstrap key. This step is useful for HA mode and this this key is just a placeholder so that other control-plane nodes do not start generating new CA certs. As we are not using HA mode in this example, this is not relevant.\\n\\n```\\nBootstrap key locked for initial create\\n```\\n\\nK3s then generates all the TLS certificates required for the internal communications:\\n\\n```\\ngenerated self-signed CA certificate\\ncertificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1742309831\\ncertificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1742309831\\ngenerated self-signed CA certificate CN=k3s-server-ca@1742309831\\ncertificate CN=kube-apiserver signed by CN=k3s-server-ca@1742309831\\ngenerated self-signed CA certificate CN=k3s-request-header-ca@1742309831\\ncertificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1742309831\\ngenerated self-signed CA certificate CN=etcd-server-ca@1742309831\\ncertificate CN=etcd-client signed by CN=etcd-server-ca@1742309831\\ngenerated self-signed CA certificate CN=etcd-peer-ca@1742309831\\ncertificate CN=etcd-peer signed by CN=etcd-peer-ca@1742309831\\ncertificate CN=etcd-server signed by CN=etcd-server-ca@1742309831\\ncertificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1742309831\\n```\\n\\nAnd then saves the bootstrap data in the bootstrap key. Again, this is not relevant for this example:\\n\\n```\\nSaving cluster bootstrap data to datastore\\n```\\n\\nAfter that, K3s starts the different Kubernetes components. These components are all run within the k3s process as goroutines, which are lightweight, concurrent functions in Go, allowing for efficient resource usage. This is another design decision taken to reduce boot time and reduce resource consumption.\\n\\n```\\nRunning kube-apiserver\\nRunning kube-scheduler\\nRunning kube-controller-manager\\n```\\n\\nManifests for packaged components are extracted to `/var/lib/rancher/k3s/server/manifests/`. When all the different Kubernetes components are running and K3s initialization is ready, the deploy controller begins watching this directory and applies all the manifests. This is how components like CoreDNS or Traefik eventually get installed.\\n\\nAnd that\u2019s it, in a short period of time, you end up with a fully deployed and running Kubernetes distribution. \ud83c\udf89\\n\\n## Conclusion \ud83c\udfc1\\n\\nThis exploration has hopefully demystified some of the initial steps that enable K3s to materialize a fully functional Kubernetes cluster. By examining the logs, we\'ve shed some light on the meaning behind each step, providing you with a deeper understanding of how K3s deploys in such a fast manner. We hope you find this knowledge useful to troubleshoot or at least to understand a bit deeper how K3s works."},{"id":"/2025/03/10/simple-ha","metadata":{"permalink":"/ja/blog/2025/03/10/simple-ha","source":"@site/blog/2025-03-10-simple-ha.md","title":"The Basic HA Cluster","description":"Creating the simplest High Availability cluster with LB and upgrading","date":"2025-03-10T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/dereknola","name":"Derek Nola","imageURL":"https://github.com/dereknola.png","key":"dereknola","page":null}],"frontMatter":{"title":"The Basic HA Cluster","description":"Creating the simplest High Availability cluster with LB and upgrading","authors":"dereknola","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"K3s initialization deep dive","permalink":"/ja/blog/2025/03/25/K3s-initialization"},"nextItem":{"title":"Hello Blog","permalink":"/ja/blog/2025/03/09/hello-blog"}},"content":"While we have more [detailed docs](/datastore/ha-embedded/) on setting up a High Availability (HA) cluster, this post will cover the simplest HA cluster you can create. \\n\\n\x3c!-- truncate --\x3e\\n## Baseline HA Cluster \ud83d\udcbb\ud83d\udda5\ufe0f\ud83d\udcbb\\n\\nWhenever we get a question around HA, this is the cluster configuration I start with. It provides a solid foundation when deploying beyond a single server.\\n\\nOur cluster will have:\\n- 4 nodes or VMs:\\n    - 1 load balancer\\n    - 3 servers\\n- A k3s-upgrade plan that will automatically update the cluster to the latest patch version of a given minor.\\n\\n## Cluster Setup \ud83c\udf10\ud83d\udd27\\n\\nI\'m using `vagrant` to provision 4 Ubuntu 24.04 VMs for this setup, all on a flat network. Setup of nodes is left as an exercise for the reader \ud83d\ude05.\\n\\nMy nodes are configured with the following names and IPs:\\n| Name | IP |\\n|------|----|\\n| lb-0 | 10.10.10.100 |\\n| server-0 | 10.10.10.50 |\\n| server-1 | 10.10.10.51 |\\n| server-2 | 10.10.10.52 |\\n\\n### Load Balancer\\n\\nI\'m using [haproxy](https://www.haproxy.org/) as it supports later expansion to multiple LB nodes (via keepalived).\\n\\nSSH into the load balancer and install haproxy:\\n\\n```bash\\nsudo apt install haproxy\\n```\\n\\nThe haproxy config is simple, just forward traffic to the servers:\\n\\n```\\n#/etc/haproxy/haproxy.cfg\\nfrontend k3s\\n    bind *:6443\\n    mode tcp\\n    default_backend k3s\\n\\nbackend k3s\\n    mode tcp\\n    option tcp-check\\n    balance roundrobin\\n    server server-0 10.10.10.50:6443 check\\n    server server-1 10.10.10.51:6443 check\\n    server server-2 10.10.10.52:6443 check\\n```\\n\\nRestart haproxy to apply the config:\\n\\n```bash\\nsystemctl restart haproxy\\n```\\n\\n### Install K3s on first server\\n\\nOn the first server, install K3s with embedded etcd and a known token:\\n\\n```bash\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=v1.31 sh -s - \\\\\\n--cluster-init --token k3sblog --tls-san 10.10.10.100\\n```\\n\\nWe pass the `--tls-san` flag adds the load balancer IP as a Subject Alternative Name (SAN) for the certificate.\\n\\n### Join the other servers\\n\\nOn the other servers, join the cluster via the load balancer:\\n\\n```bash\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=v1.31 sh -s - \\\\\\n--server https://10.10.10.100:6443 --token k3sblog\\n```\\n\\n### Grab the kubeconfig\\n\\nNow that the cluster is up, we can grab the kubeconfig from the first server:\\n\\n```bash\\nscp server-0:/etc/rancher/k3s/k3s.yaml k3s.yaml\\n```\\n\\nModify it to access the cluster via the load balancer:\\n\\n```bash\\nsed -i \'s/127.0.0.1/10.10.10.100/\' k3s.yaml\\n```\\n\\nNo we can manage the cluster from our local machine:\\n\\n```bash\\nexport KUBECONFIG=$(pwd)/k3s.yaml\\nkubectl get nodes\\n```\\n\\n## Upgrade Plan \ud83c\udfd7\ufe0f\ud83d\udcdd\ud83d\udcd0\\n\\nThe plan I\'m using will keep k3s updated to the latest patch version of the channel we give. In this case I\'m using the `v1.31` channel, the same channel used above. Kubernetes v1.31.4 just released at time of writing this post, so with this plan we have stable upgrades handled for the next 10-12 months (depending on how many patch releases this minor gets).\\n\\n### Install the system-upgrade-controller\\n\\nThe upgrade plan is managed by the system-upgrade-controller. Install it:\\n\\n```bash\\nkubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/latest/download/system-upgrade-controller.yaml\\nkubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/latest/download/crd.yaml\\n```\\n\\n### Create the upgrade plan\\n```yaml\\n#server-plan.yaml\\napiVersion: upgrade.cattle.io/v1\\nkind: Plan\\nmetadata:\\n  name: server-plan\\n  namespace: system-upgrade\\nspec:\\n  concurrency: 1\\n  cordon: true\\n  nodeSelector:\\n    matchExpressions:\\n    - key: node-role.kubernetes.io/control-plane\\n      operator: In\\n      values:\\n      - \\"true\\"\\n  serviceAccountName: system-upgrade\\n  upgrade:\\n    image: rancher/k3s-upgrade\\n  channel: https://update.k3s.io/v1-release/channels/v1.31\\n```\\n\\n```bash\\nkubectl apply -f server-plan.yaml\\n```\\n\\nSee the [automated upgrade docs](/upgrades/automated) for more details.\\n\\n\\n## Conclusion \ud83d\ude80\\n\\n![kubectl summary](kubectl.png)\\n\\nWe now have a high-availability cluster, accessible via a single IP. Upgrades are handled for the next year. This is a great starting point to:\\n- Add agent nodes to expand our workload capacity\\n- Add another load-balancer for additional redundancy"},{"id":"/2025/03/09/hello-blog","metadata":{"permalink":"/ja/blog/2025/03/09/hello-blog","source":"@site/blog/2025-03-09-hello-blog.md","title":"Hello Blog","description":"This is the first blog post on k3s.io","date":"2025-03-09T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/dereknola","name":"Derek Nola","imageURL":"https://github.com/dereknola.png","key":"dereknola","page":null}],"frontMatter":{"title":"Hello Blog","description":"This is the first blog post on k3s.io","authors":"dereknola"},"unlisted":false,"prevItem":{"title":"The Basic HA Cluster","permalink":"/ja/blog/2025/03/10/simple-ha"}},"content":"This is the first post on blog.k3s.io\\n\\n\x3c!-- everything above is seen in the snippet on the main blog page. The truncate tag below hides the rest --\x3e\\n\\n\x3c!-- truncate --\x3e\\n\\nWe will explore aspects of K3s, Kubernetes, and other related topics. These long form posts will be written by the K3s team and help illuminate aspects of the project that are not easily covered in the documentation.\\n\\nStay tuned for more posts in the future."}]}}')}}]);