"use strict";(self.webpackChunkk_3_s_docs=self.webpackChunkk_3_s_docs||[]).push([[9081],{7677:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/08/30/K3s-1.34-release","metadata":{"permalink":"/ja/blog/2025/08/30/K3s-1.34-release","source":"@site/blog/2025-08-30-K3s-1.34-release.md","title":"Kubernetes v1.34 is out!","description":"Step back and look at K3s recent work and achievements","date":"2025-08-30T00:00:00.000Z","tags":[],"hasTruncateMarker":false,"authors":[{"title":"K3s maintainer","url":"https://github.com/manuelbuil","name":"Manuel Buil","imageURL":"https://github.com/manuelbuil.png","key":"manuelbuil","page":null}],"frontMatter":{"title":"Kubernetes v1.34 is out!","description":"Step back and look at K3s recent work and achievements","authors":"manuelbuil","hide_table_of_contents":true},"unlisted":false,"nextItem":{"title":"K3s initialization deep dive","permalink":"/ja/blog/2025/03/25/K3s-initialization"}},"content":"Kubernetes 1.34 is finally here! A look back at K3s recent work and achievements \ud83e\udd73\\n\\nIt\'s been a busy and productive few months, and we\'re excited to share some of the amazing progress we\'ve made. From new features that make your lives easier to important bug fixes and foundational improvements, our team and community have been hard at work.\\n\\n## Key Features and Improvements \u2728\\nHere\u2019s a look at some of the most impactful features and updates we\'ve rolled out:\\n\\n#### Automatic Certificate Renewal Window Increase \ud83d\udcc5\\nWe\'ve made certificate management more user-friendly by increasing the automatic certificate renewal window from 90 to 120 days. This means if you perform quarterly upgrades, your certificates will be renewed more frequently, preventing them from expiring and making your clusters unusable. For more information check our [docs](https://docs.k3s.io/cli/certificate#client-and-server-certificates)\\n\\n#### Optional Airgap Image Tarball Imports \ud83d\udca8\\nFor those who use K3s in an airgap environment, you now have the option to skip importing all image tarballs. You can select to import only images that have changed since they were last imported, even across restarts. This can significantly speed up the startup process when deploying a large number of images, as the kubelet can start sooner. Check our [docs](https://docs.k3s.io/installation/airgap?airgap-load-images=Manually+Deploy+Images#enable-conditional-image-imports) for more information\\n\\n#### Enhanced Certificate Check Output \u2705\\nWe\'ve improved the output of our certificate checks to provide clearer information about certificate usage and expiration dates. The output can also now be viewed in different formats. Our [docs](https://docs.k3s.io/cli/certificate#checking-expiration-dates) show an example of the new output format.\\n\\n#### Kube-scheduler and Kube-controller-manager Certificate Management \ud83d\udd10\\nWe are now generating and managing certificates for kube-scheduler and kube-controller-manager, and they can be rotated using our existing certificate rotation [tool](https://docs.k3s.io/cli/certificate).\\n\\n#### Retention Flag for S3 Stored Snapshots \ud83d\udcbe\\nWe\'ve added a new retention flag for snapshots stored in an S3 bucket. This allows you to keep snapshots in S3 for longer periods while maintaining a smaller number of local snapshots. This is a great feature for balancing long-term disaster recovery with local storage efficiency. Check out our [docs](https://docs.k3s.io/cli/etcd-snapshot#s3-retention) for more information about how to use it.\\n\\n#### Official Governance Model \ud83e\udd1d\\nWe\'re proud to announce that we now have an official governance model in place! This will help bring more clarity to our project and hopefully encourage more developers to join our fantastic community. You can read it [here](https://github.com/k3s-io/k3s/blob/master/GOVERNANCE.md).\\n\\n## Bug Fixes and Other Notable Changes \ud83d\udee0\ufe0f\\nWe\'ve also been busy tackling a number of bugs, tech debt items and making other important improvements under the hood:\\n\\nNumerous bug fixes, including those for secrets encryption timeouts and race conditions, DNS fallbacks, various authorization and authentication handling issues and replacing go-bindata with the go native embed package.\\n\\nImprovements to our test and build infrastructure, e.g. migrating of K3s release artifacts to GitHub Actions (GHA).\\n\\n## Version bumps for key components \ud83d\ude80\\n\\nApart from Kubernetes v1.34.x, we bumped versions for several key components. Here is the list with the latest versions:\\n\\n| Component | New Version |\\n| :--- | :--- |\\n| Kine | v0.14.0 |\\n| SQLite | v3.50.4 |\\n| Etcd | v3.6.4 |\\n| Containerd | v2.1.4 |\\n| Runc | v1.3.1 |\\n| Flannel | v0.27.0 |\\n| Metrics-server | v0.8.0 |\\n| Traefik | v3.3.6 |\\n| Coredns | v1.12.3 |\\n| Helm-controller | v0.16.13 |\\n| Local-path-provisioner | v0.0.32 |\\n\\n\\n## Special Thanks to Our Contributors \ud83d\ude4f\\nWe want to give a special shout-out to the incredible contributors who are not part of our core maintainers list. Your work is invaluable to the project\'s success. Thank you to: @ErikJiang, @eggplants, @muicoder, @eugercek, @yulken, @l2dy, @OrlinVasilev\\n\\nWe look forward to an even more productive future with all of you!"},{"id":"/2025/03/25/K3s-initialization","metadata":{"permalink":"/ja/blog/2025/03/25/K3s-initialization","source":"@site/blog/2025-03-25-K3s-initialization.md","title":"K3s initialization deep dive","description":"Explain k3s initialization steps","date":"2025-03-25T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/manuelbuil","name":"Manuel Buil","imageURL":"https://github.com/manuelbuil.png","key":"manuelbuil","page":null}],"frontMatter":{"title":"K3s initialization deep dive","description":"Explain k3s initialization steps","authors":"manuelbuil","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"Kubernetes v1.34 is out!","permalink":"/ja/blog/2025/08/30/K3s-1.34-release"},"nextItem":{"title":"The Basic HA Cluster","permalink":"/ja/blog/2025/03/10/simple-ha"}},"content":"K3s is a lightweight Kubernetes distribution which excels in its deployment speed and minimal resource footprint. In fact, a lot of our users love K3s because it offers an unparalleled initialization speed.\\n\\n\x3c!-- truncate --\x3e\\nThis blog post delves into the heart of K3s\'s efficiency: its initialization process. We\'ll embark on a journey through the steps that enable K3s to materialize a fully functional Kubernetes cluster so quickly. By examining K3s\u2019s own logs, we\'ll unravel the meaning behind each step, providing you with a practical understanding of how K3s achieves its remarkable speed. This exploration not only illuminates the inner workings of K3s but also equips you with the knowledge to troubleshoot your deployments.\\n\\n## The Embedded Powerhouse \u2699\ufe0f\u26a1\\nK3s leverages `go-bindata` to embed essential Linux userspace binaries and manifests directly into its executable. This eliminates external dependencies and streamlines the deployment process. Within the K3s binary, you\'ll find core components like `runc` and `containerd`, along with the k3s-root tarball (e.g. k3s-root-amd64.tar). This tarball contains all the userspace binaries necessary for K3s to function, reducing the reliance on the host OS. If you would like all the K3s embedded binaries to take preference over the host OS binaries, you should use the `--prefer-bundled-bin` flag.\\n\\nThe embedded binaries are always deployed in the same directory: `/var/lib/rancher/k3s/data`. If you inspect this folder, you will notice it contains at least three subdirectories: `cni`, `current` and a long string of characters (or SHA). That long string of characters is generated when building K3s and it is the result of a `sha256sum` operation made on the tarball with the embedded binaries. As these change in each release, you will see a different string of characters for each release. In fact, after an upgrade, there will be two directories with a long string of characters as their name.\\n\\n`current` is just a symlink to the SHA directory and `cni` includes different cni plugins that are also symlinks to the cni binary in the SHA directory. This is because we are building all cni plugins in just one binary using multi-exec tooling. This is again a way to be more efficient and less resource consuming. If your current K3s deployment underwent an upgrade process, you will see one extra directory called `previous`, which is another symlink to the previous SHA directory. For clarification this example:\\n\\n```\\n$> ls -ahltr /var/lib/rancher/k3s/data/\\ntotal 24K\\n-rw------- 1 root root    0 Mar 19 06:22 .lock\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:22 ..\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:28 82142f5157c67effc219aeefe0bc03e0460fc62b9fbae9e901270c86b5635d53\\nlrwxrwxrwx 1 root root   90 Mar 19 06:28 previous -> /var/lib/rancher/k3s/data/82142f5157c67effc219aeefe0bc03e0460fc62b9fbae9e901270c86b5635d53\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:30 b13851fe661ab93938fc9a881cdce529da8c6b9b310b2440ef01a860f8b9c3a9\\nlrwxrwxrwx 1 root root   90 Mar 19 06:30 current -> /var/lib/rancher/k3s/data/b13851fe661ab93938fc9a881cdce529da8c6b9b310b2440ef01a860f8b9c3a9\\ndrwxr-xr-x 2 root root 4.0K Mar 19 06:30 cni\\ndrwxr-xr-x 4 root root 4.0K Mar 19 06:40 .\\n```\\n\\nAdditionally, K3s includes embedded Helm charts and manifests for deploying critical services such as CoreDNS, Traefik, and local storage. These embedded charts, formatted as yaml files, can be found in the control plane nodes in the directory: `/var/lib/rancher/k3s/server/manifests`.\\n\\nFor 67MB our K3s binary includes a lot of stuff!\\n\\n\\n## The Boot Sequence, Step-by-Step \ud83d\udc63\\nNow that we established how K3s is carrying its embedded tools, we can explore the boot up sequence. Let us look at the typical logs you can find in `journalctl` when deploying a control-plane or K3s server instance. It all starts with:\\n\\n```\\nStarting Lightweight Kubernetes...\\n```\\n\\nAnd then:\\n\\n```\\n/usr/bin/systemctl is-enabled --quiet nm-cloud-setup.service\\n```\\n\\nThis part is checking for a network manager utility which must be disabled as described in the [docs](https://docs.k3s.io/installation/requirements?_highlight=nm&_highlight=cloud&_highlight=setup.service&os=rhel#operating-systems). It configures some parts of the network stack, specifically the routing tables, which conflict with Kubernetes networking, and that is why we verify if it was correctly disabled.\\n\\nThe next message should look familiar\\n\\n```\\nAcquiring lock file /var/lib/rancher/k3s/data/.lock\\nPreparing data dir /var/lib/rancher/k3s/data/f8e9b5e7d85085972f4a9ddfd539d4dcf887be2e380a55f415c93cac5516dad5\\n```\\n\\nWhen this message is shown, the directory where K3s deploys the embedded binaries has already been created. At this point, K3s will extract the binaries. We use the lock to avoid concurrent modifications, preventing K3s embedded commands like `kubectl` or `ctr` from executing and disturbing the K3s initialization.\\n\\nThe next block of logs point at the K3s version and the datastore. In this case, I am using the default datastore which means kine with sqlite. For more information on the different datastores available check this [link](https://docs.k3s.io/datastore)\\n\\n```\\nStarting k3s v1.32.2+k3s1\\nConfiguring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s\\nConfiguring database table schema and indexes, this may take a moment...\\nDatabase tables and indexes are up to date\\nKine available at unix://kine.sock\\n```\\n\\nOnce the datastore is available k3s locks the bootstrap key. This step is useful for HA mode and this this key is just a placeholder so that other control-plane nodes do not start generating new CA certs. As we are not using HA mode in this example, this is not relevant.\\n\\n```\\nBootstrap key locked for initial create\\n```\\n\\nK3s then generates all the TLS certificates required for the internal communications:\\n\\n```\\ngenerated self-signed CA certificate\\ncertificate CN=system:admin,O=system:masters signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:k3s-supervisor,O=system:masters signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:kube-controller-manager signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:kube-scheduler signed by CN=k3s-client-ca@1742309831\\ncertificate CN=system:apiserver,O=system:masters signed by CN=k3s-client-ca@1742309831\\ncertificate CN=k3s-cloud-controller-manager signed by CN=k3s-client-ca@1742309831\\ngenerated self-signed CA certificate CN=k3s-server-ca@1742309831\\ncertificate CN=kube-apiserver signed by CN=k3s-server-ca@1742309831\\ngenerated self-signed CA certificate CN=k3s-request-header-ca@1742309831\\ncertificate CN=system:auth-proxy signed by CN=k3s-request-header-ca@1742309831\\ngenerated self-signed CA certificate CN=etcd-server-ca@1742309831\\ncertificate CN=etcd-client signed by CN=etcd-server-ca@1742309831\\ngenerated self-signed CA certificate CN=etcd-peer-ca@1742309831\\ncertificate CN=etcd-peer signed by CN=etcd-peer-ca@1742309831\\ncertificate CN=etcd-server signed by CN=etcd-server-ca@1742309831\\ncertificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1742309831\\n```\\n\\nAnd then saves the bootstrap data in the bootstrap key. Again, this is not relevant for this example:\\n\\n```\\nSaving cluster bootstrap data to datastore\\n```\\n\\nAfter that, K3s starts the different Kubernetes components. These components are all run within the k3s process as goroutines, which are lightweight, concurrent functions in Go, allowing for efficient resource usage. This is another design decision taken to reduce boot time and reduce resource consumption.\\n\\n```\\nRunning kube-apiserver\\nRunning kube-scheduler\\nRunning kube-controller-manager\\n```\\n\\nManifests for packaged components are extracted to `/var/lib/rancher/k3s/server/manifests/`. When all the different Kubernetes components are running and K3s initialization is ready, the deploy controller begins watching this directory and applies all the manifests. This is how components like CoreDNS or Traefik eventually get installed.\\n\\nAnd that\u2019s it, in a short period of time, you end up with a fully deployed and running Kubernetes distribution. \ud83c\udf89\\n\\n## Conclusion \ud83c\udfc1\\n\\nThis exploration has hopefully demystified some of the initial steps that enable K3s to materialize a fully functional Kubernetes cluster. By examining the logs, we\'ve shed some light on the meaning behind each step, providing you with a deeper understanding of how K3s deploys in such a fast manner. We hope you find this knowledge useful to troubleshoot or at least to understand a bit deeper how K3s works."},{"id":"/2025/03/10/simple-ha","metadata":{"permalink":"/ja/blog/2025/03/10/simple-ha","source":"@site/blog/2025-03-10-simple-ha.md","title":"The Basic HA Cluster","description":"Creating the simplest High Availability cluster with LB and upgrading","date":"2025-03-10T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/dereknola","name":"Derek Nola","imageURL":"https://github.com/dereknola.png","key":"dereknola","page":null}],"frontMatter":{"title":"The Basic HA Cluster","description":"Creating the simplest High Availability cluster with LB and upgrading","authors":"dereknola","hide_table_of_contents":true},"unlisted":false,"prevItem":{"title":"K3s initialization deep dive","permalink":"/ja/blog/2025/03/25/K3s-initialization"},"nextItem":{"title":"Hello Blog","permalink":"/ja/blog/2025/03/09/hello-blog"}},"content":"While we have more [detailed docs](/datastore/ha-embedded/) on setting up a High Availability (HA) cluster, this post will cover the simplest HA cluster you can create. \\n\\n\x3c!-- truncate --\x3e\\n## Baseline HA Cluster \ud83d\udcbb\ud83d\udda5\ufe0f\ud83d\udcbb\\n\\nWhenever we get a question around HA, this is the cluster configuration I start with. It provides a solid foundation when deploying beyond a single server.\\n\\nOur cluster will have:\\n- 4 nodes or VMs:\\n    - 1 load balancer\\n    - 3 servers\\n- A k3s-upgrade plan that will automatically update the cluster to the latest patch version of a given minor.\\n\\n## Cluster Setup \ud83c\udf10\ud83d\udd27\\n\\nI\'m using `vagrant` to provision 4 Ubuntu 24.04 VMs for this setup, all on a flat network. Setup of nodes is left as an exercise for the reader \ud83d\ude05.\\n\\nMy nodes are configured with the following names and IPs:\\n| Name | IP |\\n|------|----|\\n| lb-0 | 10.10.10.100 |\\n| server-0 | 10.10.10.50 |\\n| server-1 | 10.10.10.51 |\\n| server-2 | 10.10.10.52 |\\n\\n### Load Balancer\\n\\nI\'m using [haproxy](https://www.haproxy.org/) as it supports later expansion to multiple LB nodes (via keepalived).\\n\\nSSH into the load balancer and install haproxy:\\n\\n```bash\\nsudo apt install haproxy\\n```\\n\\nThe haproxy config is simple, just forward traffic to the servers:\\n\\n```\\n#/etc/haproxy/haproxy.cfg\\nfrontend k3s\\n    bind *:6443\\n    mode tcp\\n    default_backend k3s\\n\\nbackend k3s\\n    mode tcp\\n    option tcp-check\\n    balance roundrobin\\n    server server-0 10.10.10.50:6443 check\\n    server server-1 10.10.10.51:6443 check\\n    server server-2 10.10.10.52:6443 check\\n```\\n\\nRestart haproxy to apply the config:\\n\\n```bash\\nsystemctl restart haproxy\\n```\\n\\n### Install K3s on first server\\n\\nOn the first server, install K3s with embedded etcd and a known token:\\n\\n```bash\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=v1.31 sh -s - \\\\\\n--cluster-init --token k3sblog --tls-san 10.10.10.100\\n```\\n\\nWe pass the `--tls-san` flag adds the load balancer IP as a Subject Alternative Name (SAN) for the certificate.\\n\\n### Join the other servers\\n\\nOn the other servers, join the cluster via the load balancer:\\n\\n```bash\\ncurl -sfL https://get.k3s.io | INSTALL_K3S_CHANNEL=v1.31 sh -s - \\\\\\n--server https://10.10.10.100:6443 --token k3sblog\\n```\\n\\n### Grab the kubeconfig\\n\\nNow that the cluster is up, we can grab the kubeconfig from the first server:\\n\\n```bash\\nscp server-0:/etc/rancher/k3s/k3s.yaml k3s.yaml\\n```\\n\\nModify it to access the cluster via the load balancer:\\n\\n```bash\\nsed -i \'s/127.0.0.1/10.10.10.100/\' k3s.yaml\\n```\\n\\nNo we can manage the cluster from our local machine:\\n\\n```bash\\nexport KUBECONFIG=$(pwd)/k3s.yaml\\nkubectl get nodes\\n```\\n\\n## Upgrade Plan \ud83c\udfd7\ufe0f\ud83d\udcdd\ud83d\udcd0\\n\\nThe plan I\'m using will keep k3s updated to the latest patch version of the channel we give. In this case I\'m using the `v1.31` channel, the same channel used above. Kubernetes v1.31.4 just released at time of writing this post, so with this plan we have stable upgrades handled for the next 10-12 months (depending on how many patch releases this minor gets).\\n\\n### Install the system-upgrade-controller\\n\\nThe upgrade plan is managed by the system-upgrade-controller. Install it:\\n\\n```bash\\nkubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/latest/download/system-upgrade-controller.yaml\\nkubectl apply -f https://github.com/rancher/system-upgrade-controller/releases/latest/download/crd.yaml\\n```\\n\\n### Create the upgrade plan\\n```yaml\\n#server-plan.yaml\\napiVersion: upgrade.cattle.io/v1\\nkind: Plan\\nmetadata:\\n  name: server-plan\\n  namespace: system-upgrade\\nspec:\\n  concurrency: 1\\n  cordon: true\\n  nodeSelector:\\n    matchExpressions:\\n    - key: node-role.kubernetes.io/control-plane\\n      operator: In\\n      values:\\n      - \\"true\\"\\n  serviceAccountName: system-upgrade\\n  upgrade:\\n    image: rancher/k3s-upgrade\\n  channel: https://update.k3s.io/v1-release/channels/v1.31\\n```\\n\\n```bash\\nkubectl apply -f server-plan.yaml\\n```\\n\\nSee the [automated upgrade docs](/upgrades/automated) for more details.\\n\\n\\n## Conclusion \ud83d\ude80\\n\\n![kubectl summary](kubectl.png)\\n\\nWe now have a high-availability cluster, accessible via a single IP. Upgrades are handled for the next year. This is a great starting point to:\\n- Add agent nodes to expand our workload capacity\\n- Add another load-balancer for additional redundancy"},{"id":"/2025/03/09/hello-blog","metadata":{"permalink":"/ja/blog/2025/03/09/hello-blog","source":"@site/blog/2025-03-09-hello-blog.md","title":"Hello Blog","description":"This is the first blog post on k3s.io","date":"2025-03-09T00:00:00.000Z","tags":[],"hasTruncateMarker":true,"authors":[{"title":"K3s maintainer","url":"https://github.com/dereknola","name":"Derek Nola","imageURL":"https://github.com/dereknola.png","key":"dereknola","page":null}],"frontMatter":{"title":"Hello Blog","description":"This is the first blog post on k3s.io","authors":"dereknola"},"unlisted":false,"prevItem":{"title":"The Basic HA Cluster","permalink":"/ja/blog/2025/03/10/simple-ha"}},"content":"This is the first post on blog.k3s.io\\n\\n\x3c!-- everything above is seen in the snippet on the main blog page. The truncate tag below hides the rest --\x3e\\n\\n\x3c!-- truncate --\x3e\\n\\nWe will explore aspects of K3s, Kubernetes, and other related topics. These long form posts will be written by the K3s team and help illuminate aspects of the project that are not easily covered in the documentation.\\n\\nStay tuned for more posts in the future."}]}}')}}]);